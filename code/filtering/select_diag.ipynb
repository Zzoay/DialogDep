{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 14:23:34.217697: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "from typing import *\n",
    "import random\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from model.par_with_attr import ParWithAttr\n",
    "from model.base_par import DepParser\n",
    "from utils import arc_rel_loss, uas_las, to_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    train_file = '../data/train_proc.json'\n",
    "    dev_file = '../data/val_proc.json'\n",
    "    # data_file = '../aug/diag_codt/diag_train.conll'\n",
    "    plm = 'hfl/chinese-electra-180g-base-discriminator'\n",
    "    random_seed = 42\n",
    "    num_epochs = 10\n",
    "    batch_size = 256\n",
    "    plm_lr = 2e-5\n",
    "    head_lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.2\n",
    "    grad_clip = 2\n",
    "    scheduler = 'linear'\n",
    "    warmup_ratio = 0.1\n",
    "    num_early_stop = 3\n",
    "    max_length = 128\n",
    "    hidden_size = 400\n",
    "    num_labels = 35\n",
    "    # print_every = 400\n",
    "    # eval_every = 800\n",
    "    print_every = 2000\n",
    "    eval_every = 4000\n",
    "    cuda = True\n",
    "    fp16 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dct = {\n",
    "    'root': '根节点',\n",
    "    'sasubj-obj': '同主同宾',\n",
    "    'sasubj': '同主语',\n",
    "    'dfsubj': '不同主语',\n",
    "    'subj': '主语',\n",
    "    'subj-in': '内部主语',\n",
    "    'obj': '宾语',\n",
    "    'pred': '谓语',\n",
    "    'att': '定语',\n",
    "    'adv': '状语',\n",
    "    'cmp': '补语',\n",
    "    'coo': '并列',\n",
    "    'pobj': '介宾',\n",
    "    'iobj': '间宾',\n",
    "    'de': '的',\n",
    "    'adjct': '附加',\n",
    "    'app': '称呼',\n",
    "    'exp': '解释',\n",
    "    'punc': '标点',\n",
    "    'frag': '片段',\n",
    "    'repet': '重复',\n",
    "    # rst\n",
    "    'attr': '归属',\n",
    "    'bckg': '背景',\n",
    "    'cause': '因果',\n",
    "    'comp': '比较',\n",
    "    'cond': '状况',\n",
    "    'cont': '对比',\n",
    "    'elbr': '阐述',\n",
    "    'enbm': '目的',\n",
    "    'eval': '评价',\n",
    "    'expl': '解释-例证',\n",
    "    'joint': '联合',\n",
    "    'manner': '方式',\n",
    "    'rstm': '重申',\n",
    "    'temp': '时序',\n",
    "    # 'tp-chg': '主题变更',\n",
    "    # 'prob-sol': '问题-解决',\n",
    "    # 'qst-ans': '疑问-回答',\n",
    "    # 'stm-rsp': '陈述-回应',\n",
    "    # 'req-proc': '需求-处理',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 0, 'sasubj-obj': 1, 'sasubj': 2, 'dfsubj': 3, 'subj': 4, 'subj-in': 5, 'obj': 6, 'pred': 7, 'att': 8, 'adv': 9, 'cmp': 10, 'coo': 11, 'pobj': 12, 'iobj': 13, 'de': 14, 'adjct': 15, 'app': 16, 'exp': 17, 'punc': 18, 'frag': 19, 'repet': 20, 'attr': 21, 'bckg': 22, 'cause': 23, 'comp': 24, 'cond': 25, 'cont': 26, 'elbr': 27, 'enbm': 28, 'eval': 29, 'expl': 30, 'joint': 31, 'manner': 32, 'rstm': 33, 'temp': 34}\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "rel2id = {}\n",
    "for i, (key, value) in enumerate(rel_dct.items()):\n",
    "    rel2id[key] = i\n",
    "print(rel2id)\n",
    "print(len(rel2id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['root', 'sasubj-obj', 'sasubj', 'dfsubj', 'subj', 'subj-in', 'obj', 'pred', 'att', 'adv', 'cmp', 'coo', 'pobj', 'iobj', 'de', 'adjct', 'app', 'exp', 'punc', 'frag', 'repet', 'attr', 'bckg', 'cause', 'comp', 'cond', 'cont', 'elbr', 'enbm', 'eval', 'expl', 'joint', 'manner', 'rstm', 'temp']\n"
     ]
    }
   ],
   "source": [
    "id2rel = list(rel_dct.keys())\n",
    "print(id2rel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seed & Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=CFG.random_seed):\n",
    "    np.random.seed(seed%(2**32-1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependency():\n",
    "    def __init__(self, idx, word, head, rel):\n",
    "        self.id = idx\n",
    "        self.word = word\n",
    "        self.head = head\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self):\n",
    "        # example:  1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        values = [str(self.id), self.word, \"_\", \"_\", \"_\", \"_\", str(self.head), self.rel, \"_\", \"_\"]\n",
    "        return '\\t'.join(values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.word}, {self.head}, {self.rel})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_codt(data_file: str):\n",
    "    # id, form, tag, head, rel\n",
    "#     sentence:List[Dependency] = [Dependency('0', '<root>', '_', '0', '_')]\n",
    "    sentence:List[Dependency] = []\n",
    "    \n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        # data example: 1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        for line in f.readlines():\n",
    "            toks = line.split()\n",
    "            if len(toks) == 0:\n",
    "                yield sentence\n",
    "#                 sentence = [Dependency('0', '<root>', '_', '0', '_')]\n",
    "                sentence = []\n",
    "            elif len(toks) == 10:\n",
    "                dep = Dependency(toks[0], toks[1], toks[3], toks[6], toks[7])\n",
    "                sentence.append(dep)\n",
    "\n",
    "def load_codt_with_aug(data_file: str):\n",
    "    # id, form, tag, head, rel\n",
    "#     sentence:List[Dependency] = [Dependency('0', '<root>', '_', '0', '_')]\n",
    "    sentence:List[Dependency] = []\n",
    "\n",
    "    f1 = open(data_file, 'r', encoding='utf-8')\n",
    "    f2 = open('../aug/codt/codt_train_fixed.conll')\n",
    "    for line in chain(f1.readlines(), f2.readlines()):\n",
    "        toks = line.split()\n",
    "        if len(toks) == 0:\n",
    "            yield sentence\n",
    "#                 sentence = [Dependency('0', '<root>', '_', '0', '_')]\n",
    "            sentence = []\n",
    "        elif len(toks) == 10:\n",
    "            dep = Dependency(toks[0], toks[1], toks[3], toks[6], toks[7])\n",
    "            sentence.append(dep)\n",
    "\n",
    "    f1.close()\n",
    "    f2.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annoted(data_file):\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    sample_lst:List[List[Dependency]] = []\n",
    "    \n",
    "    for d in data:\n",
    "        rel_dct = {}\n",
    "        for tripple in d['relationship']:\n",
    "            head, rel, tail = tripple\n",
    "            head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "            tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "            if head_uttr_idx != tail_uttr_idx:\n",
    "                continue\n",
    "            \n",
    "            if not rel_dct.get(head_uttr_idx, None):\n",
    "                rel_dct[head_uttr_idx] = {tail_word_idx: [head_word_idx, rel]}\n",
    "            else:\n",
    "                rel_dct[head_uttr_idx][tail_word_idx] = [head_word_idx, rel]\n",
    "            \n",
    "        for item in d['dialog']:\n",
    "            turn = item['turn']\n",
    "            utterance = item['utterance']\n",
    "            # dep_lst:List[Dependency] = [Dependency(0, '[root]', -1, '_')]\n",
    "            dep_lst:List[Dependency] = []\n",
    "            \n",
    "            for word_idx, word in enumerate(utterance.split(' ')):\n",
    "                # some word annoted missed, padded with last word and 'adjct'\n",
    "                head_word_idx, rel = rel_dct.get(turn, {word_idx + 1: [word_idx, 'adjct']}).get(word_idx + 1, [word_idx, 'adjct'])  \n",
    "                dep_lst.append(Dependency(word_idx + 1, word, head_word_idx, rel))  # start from 1\n",
    "            \n",
    "            sample_lst.append(dep_lst)\n",
    "        \n",
    "    return sample_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.plm)\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236590\n"
     ]
    }
   ],
   "source": [
    "data = [d for d in load_annoted(CFG.train_file)]\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CODTDataset(Dataset):\n",
    "    def __init__(self, cfg, data, train):\n",
    "        self.train = train\n",
    "        self.cfg = cfg\n",
    "        self.inputs, self.offsets, self.heads, self.rels, self.masks = self.read_data(data)\n",
    "        \n",
    "    def read_data(self, data):\n",
    "        inputs, offsets = [], []\n",
    "        tags, heads, rels, masks = [], [], [], []\n",
    "        \n",
    "        file = self.cfg.train_file if self.train else self.cfg.dev_file\n",
    "        \n",
    "        for deps in tqdm(data):\n",
    "            # another sentence\n",
    "            seq_len = len(deps)\n",
    "            \n",
    "            word_lst = [] \n",
    "            rel_attr = {'input_ids':torch.Tensor(), 'token_type_ids':torch.Tensor(), 'attention_mask':torch.Tensor()}\n",
    "            head_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)  # same as root index is 0, constrainting by mask \n",
    "            rel_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            mask_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            for i, dep in enumerate(deps):\n",
    "                if i == seq_len or i + 1== self.cfg.max_length:\n",
    "                    break\n",
    "                    \n",
    "                word_lst.append(dep.word)\n",
    "                    \n",
    "                if dep.head in ['_', '-1'] or int(dep.head) + 1 >= self.cfg.max_length:\n",
    "                    head_tokens[i+1] = 0\n",
    "                    mask_tokens[i+1] = 0\n",
    "                else:\n",
    "                    head_tokens[i+1] = int(dep.head)\n",
    "                    mask_tokens[i+1] = 1\n",
    "\n",
    "                if self.train:\n",
    "                    rel_tokens[i+1] = rel2id[dep.rel]\n",
    "                else:\n",
    "                    rel_tokens[i+1] = rel2id.get(dep.rel, rel2id['adjct'])\n",
    "\n",
    "            tokenized = tokenizer.encode_plus(word_lst, \n",
    "                                                padding='max_length', \n",
    "                                                truncation=True,\n",
    "                                                max_length=self.cfg.max_length, \n",
    "                                                return_offsets_mapping=True, \n",
    "                                                return_tensors='pt',\n",
    "                                                is_split_into_words=True)\n",
    "            inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "                            \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                            \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                            })\n",
    "            \n",
    "#                 sentence_word_idx = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            sentence_word_idx = []\n",
    "            for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "                if start == 0 and end != 0:\n",
    "                    sentence_word_idx.append(idx)\n",
    "#                         sentence_word_idx[idx] = idx\n",
    "            if len(sentence_word_idx) < self.cfg.max_length - 1:\n",
    "                sentence_word_idx.extend([0]* (self.cfg.max_length - 1 - len(sentence_word_idx)))\n",
    "            offsets.append(torch.as_tensor(sentence_word_idx))\n",
    "            \n",
    "            heads.append(head_tokens)\n",
    "            rels.append(rel_tokens)\n",
    "            masks.append(mask_tokens)\n",
    "                    \n",
    "        return inputs, offsets, heads, rels, masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.offsets[idx], self.heads[idx], self.rels[idx], self.masks[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236590/236590 [06:08<00:00, 642.88it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = CODTDataset(CFG, data, train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "model = DepParser(CFG)\n",
    "print(model.load_state_dict(torch.load('../results/few_shot/with_codt/model.bin')))\n",
    "\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 585/925 [08:49<08:09,  1.44s/it]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataloader = DataLoader(dataset, batch_size=CFG.batch_size)\n",
    "\n",
    "arc_logits, rel_logits = torch.Tensor(), torch.Tensor()\n",
    "heads_whole, rels_whole, masks_whole = torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "sampled_ids, accum = torch.Tensor(), 0\n",
    "for batch in tqdm(dataloader):\n",
    "    inputs, offsets, heads, rels, masks = batch\n",
    "    \n",
    "    inputs_cuda = {}\n",
    "    for key, value in inputs.items():\n",
    "        inputs_cuda[key] = value.cuda()\n",
    "    inputs = inputs_cuda\n",
    "\n",
    "    offsets, heads, rels, masks = to_cuda(data=(offsets, heads, rels, masks))\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        arc_logit, rel_logit = model(inputs, offsets, masks=masks, heads=None, rels=None, evaluate=True)\n",
    "        # arc_logit, rel_logit, similarity = model.predict(inputs, offsets, masks)\n",
    "    \n",
    "    arc_logit = arc_logit.softmax(-1)\n",
    "    rel_logit = rel_logit.softmax(-1)\n",
    "    \n",
    "    arc_logit_masked = arc_logit * masks.unsqueeze(-1).expand(-1, -1, arc_logit.size()[-1])\n",
    "    rel_logit_masked = rel_logit * masks.unsqueeze(-1).expand(-1, -1, rel_logit.size()[-1])\n",
    "    \n",
    "    threshold = .98\n",
    "\n",
    "    arc_sampled = (arc_logit_masked.max(-1)[0].sum(-1) / masks.sum(-1)) > threshold\n",
    "    rel_sampled = (rel_logit_masked.max(-1)[0].sum(-1) / masks.sum(-1)) > threshold\n",
    "    sampled = arc_sampled * rel_sampled\n",
    "    sampled_idx = sampled.nonzero().squeeze(1)\n",
    "    \n",
    "    arc_logit = arc_logit[sampled_idx, :, :]\n",
    "    rel_logit = rel_logit[sampled_idx, :, :]\n",
    "    masks = masks[sampled_idx, :]\n",
    "    \n",
    "    arc_logit[:, torch.arange(arc_logit.size()[1]), torch.arange(arc_logit.size()[2])] = -1e4\n",
    "    \n",
    "    sampled_ids = torch.cat([sampled_ids, sampled_idx.cpu().int() + accum])\n",
    "    accum += CFG.batch_size\n",
    "        \n",
    "    arc_logits = torch.cat([arc_logits, arc_logit.cpu()])\n",
    "    rel_logits = torch.cat([rel_logits, rel_logit.cpu()])\n",
    "    masks_whole = torch.cat([masks_whole, masks.cpu()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([75521, 128, 128])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arc_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_ids = sampled_ids.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_preds = arc_logits.argmax(-1)\n",
    "rel_preds = rel_logits.argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('../diag_train_sampled.conll', 'w+', encoding='utf-8')\n",
    "\n",
    "output = []\n",
    "cnt = 0\n",
    "for i, d in enumerate(data):\n",
    "    if i in sampled_ids:\n",
    "        for idx, dep in enumerate(d):\n",
    "            head = arc_preds[cnt][idx+1].item()\n",
    "            rel = id2rel[rel_preds[cnt][idx+1].item()]\n",
    "            \n",
    "            save_str = f'{dep.id}\\t{dep.word}\\t_\\t_\\t_\\t_\\t{head}\\t{rel}\\t_\\t_\\n'\n",
    "            fw.write(save_str)\n",
    "        \n",
    "        fw.write('\\n')\n",
    "        cnt += 1\n",
    "        # break\n",
    "        \n",
    "fw.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py395",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 14:42:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1adbcecb1e586d9b8eb419680ab9509109c36838db5c3be5530ca5bad501e730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
