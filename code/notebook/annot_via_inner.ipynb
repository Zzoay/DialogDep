{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d08ca0d5-c675-4aee-881b-219e5af18b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import *\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "430b0cd3-4d5e-4bd5-8b0c-783b2c98b55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_constant_schedule_with_warmup, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "046935a1-534d-44a6-95b2-fb7bee09a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import DepParser\n",
    "from utils import arc_rel_loss, uas_las, to_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cdb354e-5434-4591-851f-3684b33b0821",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b464d9a6-1134-44f0-879e-b38ba94006de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_file = '/root/diag_dep/data_testset/1to500_1013.json'\n",
    "    plm = 'hfl/chinese-electra-180g-large-discriminator'\n",
    "    num_folds = 5\n",
    "    trn_folds = [0, 1, 2, 3, 4]\n",
    "    random_seed = 42\n",
    "    num_epochs = 10\n",
    "    batch_size = 128\n",
    "    lr = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.2\n",
    "    grad_clip = 1\n",
    "    scheduler = 'linear'\n",
    "    warmup_ratio = 0.1\n",
    "    num_early_stop = 5\n",
    "    max_length = 160\n",
    "    num_labels = 40\n",
    "    hidden_size = 400\n",
    "    print_every = 1e9\n",
    "    eval_every = 50\n",
    "    cuda = True\n",
    "    fp16 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0f511c-3577-45b5-8ef9-c8db0f384199",
   "metadata": {},
   "source": [
    "## Seed and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "022cb54a-46c6-4e25-af26-2ac04f34ef49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=CFG.random_seed):\n",
    "    np.random.seed(seed%(2**32-1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ae76a3-8dd6-4e8d-85d7-fd539fe9c46c",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc85d24f-4b1a-40f1-9e33-4efff938c5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dct = {\n",
    "    'root': '根节点',\n",
    "    'sasubj-obj': '同主同宾',\n",
    "    'sasubj': '同主语',\n",
    "    'dfsubj': '不同主语',\n",
    "    'subj': '主语',\n",
    "    'subj-in': '内部主语',\n",
    "    'obj': '宾语',\n",
    "    'pred': '谓语',\n",
    "    'att': '定语',\n",
    "    'adv': '状语',\n",
    "    'cmp': '补语',\n",
    "    'coo': '并列',\n",
    "    'pobj': '介宾',\n",
    "    'iobj': '间宾',\n",
    "    'de': '的',\n",
    "    'adjct': '附加',\n",
    "    'app': '称呼',\n",
    "    'exp': '解释',\n",
    "    'punc': '标点',\n",
    "    'frag': '片段',\n",
    "    'repet': '重复',\n",
    "    # rst\n",
    "    'attr': '归属',\n",
    "    'bckg': '背景',\n",
    "    'cause': '因果',\n",
    "    'comp': '比较',\n",
    "    'cond': '状况',\n",
    "    'cont': '对比',\n",
    "    'elbr': '阐述',\n",
    "    'enbm': '目的',\n",
    "    'eval': '评价',\n",
    "    'expl': '解释-例证',\n",
    "    'joint': '联合',\n",
    "    'manner': '方式',\n",
    "    'rstm': '重申',\n",
    "    'temp': '时序',\n",
    "    'tp-chg': '主题变更',\n",
    "    'prob-sol': '问题-解决',\n",
    "    'qst-ans': '疑问-回答',\n",
    "    'stm-rsp': '陈述-回应',\n",
    "    'req-proc': '需求-处理',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dee510c1-c7d9-4f40-a7e6-7553b1587502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 0, 'sasubj-obj': 1, 'sasubj': 2, 'dfsubj': 3, 'subj': 4, 'subj-in': 5, 'obj': 6, 'pred': 7, 'att': 8, 'adv': 9, 'cmp': 10, 'coo': 11, 'pobj': 12, 'iobj': 13, 'de': 14, 'adjct': 15, 'app': 16, 'exp': 17, 'punc': 18, 'frag': 19, 'repet': 20, 'attr': 21, 'bckg': 22, 'cause': 23, 'comp': 24, 'cond': 25, 'cont': 26, 'elbr': 27, 'enbm': 28, 'eval': 29, 'expl': 30, 'joint': 31, 'manner': 32, 'rstm': 33, 'temp': 34, 'tp-chg': 35, 'prob-sol': 36, 'qst-ans': 37, 'stm-rsp': 38, 'req-proc': 39}\n",
      "['root', 'sasubj-obj', 'sasubj', 'dfsubj', 'subj', 'subj-in', 'obj', 'pred', 'att', 'adv', 'cmp', 'coo', 'pobj', 'iobj', 'de', 'adjct', 'app', 'exp', 'punc', 'frag', 'repet', 'attr', 'bckg', 'cause', 'comp', 'cond', 'cont', 'elbr', 'enbm', 'eval', 'expl', 'joint', 'manner', 'rstm', 'temp', 'tp-chg', 'prob-sol', 'qst-ans', 'stm-rsp', 'req-proc']\n"
     ]
    }
   ],
   "source": [
    "rel2id = {key:idx for idx, key in enumerate(rel_dct.keys())}\n",
    "print(rel2id)\n",
    "\n",
    "id2rel = [key for key in rel_dct.keys()]\n",
    "print(id2rel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253d336d-ff6f-4372-bc3f-98edd10434e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "add token: [root] 21128\n",
      "add token: [qst] 21129\n",
      "add token: [ans] 138\n",
      "21131\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.plm)\n",
    "print(len(tokenizer))\n",
    " \n",
    "num_added_toks = tokenizer.add_tokens(['[root]', '[qst]', '[aws]'], special_tokens=True)\n",
    "tokenizer.root_token = '[root]'\n",
    "tokenizer.root_token_ids = tokenizer('[root]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.root_token} {tokenizer.root_token_ids}\")\n",
    "\n",
    "tokenizer.qst_token = '[qst]'\n",
    "tokenizer.qst_token_ids = tokenizer('[qst]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.qst_token} {tokenizer.qst_token_ids}\")\n",
    "\n",
    "tokenizer.ans_token = '[ans]'\n",
    "tokenizer.ans_token_ids = tokenizer('[ans]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.ans_token} {tokenizer.ans_token_ids}\")\n",
    "print(len(tokenizer))\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "33a5f0e9-f45d-44fa-81cd-f5e8633c9560",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependency():\n",
    "    def __init__(self, idx, word, head, rel):\n",
    "        self.id = idx\n",
    "        self.word = word\n",
    "        self.tag = '_'\n",
    "        self.head = head\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self):\n",
    "        # example:  1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        values = [str(self.idx), self.word, \"_\", self.tag, \"_\", \"_\", str(self.head), self.rel, \"_\", \"_\"]\n",
    "        return '\\t'.join(values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.id}, {self.word}, {self.tag}, {self.head}, {self.rel})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4e38f41-b0b6-42e6-8a34-06876d29589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file, get_data_split=False):\n",
    "    with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)[500:]  # have annotated 500 data\n",
    "        \n",
    "    sample_lst:List[List[Dependency]] = []\n",
    "    split_ids:List[int] = []\n",
    "    \n",
    "    for d in data:\n",
    "        rel_dct = {}\n",
    "        for tripple in d['relationship']:\n",
    "            head, rel, tail = tripple\n",
    "            head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "            tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "            if head_uttr_idx != tail_uttr_idx:\n",
    "                continue\n",
    "            \n",
    "            if not rel_dct.get(head_uttr_idx, None):\n",
    "                rel_dct[head_uttr_idx] = {tail_word_idx: [head_word_idx, rel]}\n",
    "            else:\n",
    "                rel_dct[head_uttr_idx][tail_word_idx] = [head_word_idx, rel]\n",
    "            \n",
    "        for idx, item in enumerate(d['dialog']):\n",
    "            turn = item['turn']\n",
    "            utterance = item['utterance']\n",
    "            # dep_lst:List[Dependency] = [Dependency(0, '[root]', -1, '_')]\n",
    "            dep_lst:List[Dependency] = []\n",
    "            \n",
    "            for word_idx, word in enumerate(utterance.split(' ')):\n",
    "                head_word_idx, rel = rel_dct[turn].get(word_idx + 1, [word_idx, 'adjct'])  # some word annoted missed, padded with last word and 'adjct'\n",
    "                dep_lst.append(Dependency(word_idx + 1, word, head_word_idx, rel))  # start from 1\n",
    "            \n",
    "            sample_lst.append(dep_lst)\n",
    "        split_ids.append(idx + 1)\n",
    "     \n",
    "    if get_data_split:\n",
    "        return sample_lst, split_ids\n",
    "    return sample_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0119ca92-78fa-46ae-80f4-0e7b82345ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.inputs, self.offsets, self.heads, self.rels, self.masks = self.read_data()\n",
    "        \n",
    "    def read_data(self):\n",
    "        inputs, offsets = [], []\n",
    "        tags, heads, rels, masks = [], [], [], []\n",
    "        \n",
    "        for deps in load_data(self.cfg.data_file):\n",
    "            # another sentence\n",
    "            seq_len = len(deps)\n",
    "\n",
    "            word_lst = [] \n",
    "#                 head_tokens = np.ones(self.cfg.max_length, dtype=np.int64)*(-1)  # root index is 0, thus using -1 for padding \n",
    "            head_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)  # same as root index is 0, constrainting by mask \n",
    "            rel_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            mask_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            for i, dep in enumerate(deps):\n",
    "                if i == seq_len or i + 1== self.cfg.max_length:\n",
    "                    break\n",
    "\n",
    "                word_lst.append(dep.word)\n",
    "\n",
    "                if dep.head == -1 or dep.head + 1 >= self.cfg.max_length:\n",
    "                    head_tokens[i+1] = 0\n",
    "                    mask_tokens[i+1] = 0\n",
    "                else:\n",
    "                    head_tokens[i+1] = int(dep.head)\n",
    "                    mask_tokens[i+1] = 1\n",
    "#                     head_tokens[i] = dep.head if dep.head != '_' else 0\n",
    "                rel_tokens[i+1] = rel2id.get(dep.rel, 0)\n",
    "\n",
    "            tokenized = tokenizer.encode_plus(word_lst, \n",
    "                                              padding='max_length', \n",
    "                                              truncation=True,\n",
    "                                              max_length=self.cfg.max_length, \n",
    "                                              return_offsets_mapping=True, \n",
    "                                              return_tensors='pt',\n",
    "                                              is_split_into_words=True)\n",
    "            inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "                          \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                           \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                          })\n",
    "\n",
    "#                 sentence_word_idx = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            sentence_word_idx = []\n",
    "            for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "                if start == 0 and end != 0:\n",
    "                    sentence_word_idx.append(idx)\n",
    "#                         sentence_word_idx[idx] = idx\n",
    "            if len(sentence_word_idx) < self.cfg.max_length - 1:\n",
    "                sentence_word_idx.extend([0]* (self.cfg.max_length - 1 - len(sentence_word_idx)))\n",
    "            offsets.append(torch.as_tensor(sentence_word_idx))\n",
    "#                 offsets.append(sentence_word_idx)\n",
    "\n",
    "            heads.append(head_tokens)\n",
    "            rels.append(rel_tokens)\n",
    "            masks.append(mask_tokens)\n",
    "                    \n",
    "        return inputs, offsets, heads, rels, masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.offsets[idx], self.heads[idx], self.rels[idx], self.masks[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce824d0-e469-4df1-9c55-c179e8388549",
   "metadata": {},
   "source": [
    "## prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3722c403-b060-4000-b9e2-50966a302a00",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "--------------------------------\n",
      "7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  3,  ..., -1, -1, -1],\n",
      "        [-1,  2,  3,  ..., -1, -1, -1],\n",
      "        [-1,  3,  1,  ..., -1, -1, -1]])\n",
      "tensor([[-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  9,  ..., -1, -1, -1],\n",
      "        [-1,  8,  9,  ..., -1, -1, -1],\n",
      "        [-1, 16, 18,  ..., -1, -1, -1]])\n",
      "FOLD 1\n",
      "--------------------------------\n",
      "7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  3,  ..., -1, -1, -1],\n",
      "        [-1,  2,  3,  ..., -1, -1, -1],\n",
      "        [-1,  3,  1,  ..., -1, -1, -1]])\n",
      "tensor([[-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  9,  ..., -1, -1, -1],\n",
      "        [-1,  8,  9,  ..., -1, -1, -1],\n",
      "        [-1, 16, 18,  ..., -1, -1, -1]])\n",
      "FOLD 2\n",
      "--------------------------------\n",
      "7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  3,  ..., -1, -1, -1],\n",
      "        [-1,  2,  3,  ..., -1, -1, -1],\n",
      "        [-1,  3,  1,  ..., -1, -1, -1]])\n",
      "tensor([[-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  9,  ..., -1, -1, -1],\n",
      "        [-1,  8,  9,  ..., -1, -1, -1],\n",
      "        [-1, 16, 18,  ..., -1, -1, -1]])\n",
      "FOLD 3\n",
      "--------------------------------\n",
      "7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  3,  ..., -1, -1, -1],\n",
      "        [-1,  2,  3,  ..., -1, -1, -1],\n",
      "        [-1,  3,  1,  ..., -1, -1, -1]])\n",
      "tensor([[-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  9,  ..., -1, -1, -1],\n",
      "        [-1,  8,  9,  ..., -1, -1, -1],\n",
      "        [-1, 16, 18,  ..., -1, -1, -1]])\n",
      "FOLD 4\n",
      "--------------------------------\n",
      "7601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  3,  ..., -1, -1, -1],\n",
      "        [-1,  2,  3,  ..., -1, -1, -1],\n",
      "        [-1,  3,  1,  ..., -1, -1, -1]])\n",
      "tensor([[-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  9,  ..., -1, -1, -1],\n",
      "        [-1,  8,  9,  ..., -1, -1, -1],\n",
      "        [-1, 16, 18,  ..., -1, -1, -1]])\n",
      "-----Argmax the Mean of all fold's Logits-----\n",
      "tensor([[-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        [-1,  0,  1,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  3,  ..., -1, -1, -1],\n",
      "        [-1,  2,  3,  ..., -1, -1, -1],\n",
      "        [-1,  3,  1,  ..., -1, -1, -1]])\n",
      "tensor([[-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0, 15,  ..., -1, -1, -1],\n",
      "        [-1,  0,  6,  ..., -1, -1, -1],\n",
      "        ...,\n",
      "        [-1,  0,  9,  ..., -1, -1, -1],\n",
      "        [-1,  8,  9,  ..., -1, -1, -1],\n",
      "        [-1, 16, 18,  ..., -1, -1, -1]])\n"
     ]
    }
   ],
   "source": [
    "arc_logits_avg, rel_logits_avg = None, None\n",
    "\n",
    "for fold in CFG.trn_folds:\n",
    "    print(f'FOLD {fold}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    if CFG.cuda and torch.cuda.is_available:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # data loading\n",
    "    test_dataset = DialogDataset(CFG)\n",
    "    print(len(test_dataset))\n",
    "    test_iter = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False)\n",
    "                                 \n",
    "    model = DepParser(CFG)\n",
    "    model.load_state_dict(torch.load(f'/root/autodl-tmp/diag_dep/1to500-inner/{fold}/model.bin'))\n",
    "    model.to('cuda' if CFG.cuda else 'cpu')\n",
    "    model.eval()\n",
    "    \n",
    "    arc_logits_onefold, rel_logits_onefold, masks_onefold = torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "    for batch in test_iter:\n",
    "        inputs, offsets, heads, rels, masks = batch\n",
    "\n",
    "        if CFG.cuda and torch.cuda.is_available():\n",
    "            inputs_cuda = {}\n",
    "            for key,value in inputs.items():\n",
    "                inputs_cuda[key] = value.cuda()\n",
    "            inputs = inputs_cuda\n",
    "        offsets, heads, rels, masks = to_cuda(data=(offsets, heads, rels, masks))\n",
    "        masks = (masks == 0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            arc_logits, rel_logits = model(inputs, heads, offsets, evaluate=True)\n",
    "            \n",
    "        batch_size, seq_len, seq_len = arc_logits.shape \n",
    "        \n",
    "        # diag\n",
    "        arc_logits[:, torch.arange(seq_len), torch.arange(seq_len)] = -1e4\n",
    "        \n",
    "#         # one and only one root\n",
    "#         root_masks = torch.full_like(input=arc_logits[:, :, 0], fill_value=True, dtype=torch.bool)\n",
    "#         root_argmax = arc_logits[:, :, 0].argmax(dim=1)\n",
    "#         root_masks[torch.arange(batch_size), root_argmax] = False\n",
    "#         arc_logits[:, :, 0].masked_fill_(mask=root_masks, value=-1e4)\n",
    "#         arc_logits[:, :, 0].masked_fill_(mask=~root_masks, value=1e4)\n",
    "        \n",
    "#         root_masks = torch.full_like(input=rel_logits[:, :, 0], fill_value=True, dtype=torch.bool)\n",
    "#         root_argmax = rel_logits[:, :, 0].argmax(dim=1)\n",
    "#         root_masks[torch.arange(batch_size), root_argmax] = False\n",
    "#         rel_logits[:, :, 0].masked_fill_(mask=root_masks, value=-1e4)\n",
    "#         rel_logits[:, :, 0].masked_fill_(mask=~root_masks, value=1e4)\n",
    "        \n",
    "        masks, arc_logits, rel_logits = masks.cpu(), arc_logits.cpu(), rel_logits.cpu()\n",
    "        masks_onefold = torch.cat([masks_onefold, masks], dim=0)\n",
    "        arc_logits_onefold = torch.cat([arc_logits_onefold, arc_logits], dim=0)\n",
    "        rel_logits_onefold = torch.cat([rel_logits_onefold, rel_logits], dim=0)\n",
    "        \n",
    "    head_preds = arc_logits_onefold.argmax(-1)\n",
    "    head_preds.masked_fill_(masks_onefold.bool(), value=-1)\n",
    "    # head_preds = head_preds[1:].tolist()\n",
    "\n",
    "    rel_preds = rel_logits_onefold.argmax(-1)\n",
    "    rel_preds.masked_fill_(masks_onefold.bool(), value=-1)\n",
    "    # rel_preds = rel_preds[1:].tolist()\n",
    "\n",
    "    print(head_preds)\n",
    "    print(rel_preds)\n",
    "    \n",
    "    if arc_logits_avg is None:\n",
    "        arc_logits_avg = arc_logits_onefold / len(CFG.trn_folds)\n",
    "        rel_logits_avg = rel_logits_onefold / len(CFG.trn_folds)\n",
    "    else:\n",
    "        arc_logits_avg += arc_logits_onefold / len(CFG.trn_folds)\n",
    "        rel_logits_avg += rel_logits_onefold / len(CFG.trn_folds)\n",
    "\n",
    "head_preds = arc_logits_avg.argmax(-1)\n",
    "head_preds.masked_fill_(masks_onefold.bool(), value=-1)\n",
    "# head_preds = head_preds[1:].tolist()\n",
    "\n",
    "rel_preds = rel_logits_avg.argmax(-1)\n",
    "rel_preds.masked_fill_(masks_onefold.bool(), value=-1)\n",
    "# rel_preds = rel_preds[1:].tolist()\n",
    "\n",
    "print(\"-----Argmax the Mean of all fold's Logits-----\")\n",
    "print(head_preds)\n",
    "print(rel_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9c3d946-1526-4c23-ae3a-9d69688c0400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152\n"
     ]
    }
   ],
   "source": [
    "data, split_ids = load_data(CFG.data_file, get_data_split=True)\n",
    "max_len = 0\n",
    "for d in data:\n",
    "    max_len = max(max_len, len(d))\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71806bb1-4070-47be-ba0d-4f37880b56a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "    output = json.load(f)  \n",
    "\n",
    "data, split_ids = load_data(CFG.data_file, get_data_split=True)\n",
    "\n",
    "uttr_cnt, dialog_cnt = 0, 0\n",
    "split_idx = split_ids.pop(0)\n",
    "tripples = []\n",
    "\n",
    "for i, d in enumerate(data):\n",
    "    # print(d)\n",
    "    head_pred, rel_pred = head_preds[i], rel_preds[i]\n",
    "    # print(head_pred)\n",
    "    # print(rel_pred)\n",
    "\n",
    "    for dep in d:\n",
    "        tail = f'{uttr_cnt}-{dep.id}'\n",
    "        head_word_idx = head_pred[dep.id].item()\n",
    "        # try:\n",
    "        #     head = f'{uttr_cnt}-{head_pred[dep.id].item()}'\n",
    "        #     rel = id2rel[rel_pred[dep.id].item()]\n",
    "        # except IndexError:\n",
    "        #     head = f'{uttr_cnt}-{head_pred[dep.id - 1].item()}'\n",
    "        #     rel = id2rel[rel_pred[dep.id - 1].item()]\n",
    "        head = f'{uttr_cnt}-{head_word_idx}'\n",
    "        rel = id2rel[rel_pred[dep.id].item()]\n",
    "        if head_word_idx <= len(d):\n",
    "            tripples.append([head, rel, tail])\n",
    "    \n",
    "    uttr_cnt += 1\n",
    "    if uttr_cnt == split_idx:\n",
    "        # 500 is the index of dialog which is not annnoted\n",
    "        output[dialog_cnt + 500]['relationship'] = tripples\n",
    "        # print(tripples)\n",
    "        dialog_cnt += 1\n",
    "        tripples = []\n",
    "        uttr_cnt = 0\n",
    "        if len(split_ids) != 0:\n",
    "            split_idx = split_ids.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f5bb7148-837c-4469-bc10-4aeb959af55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fw = open('pre_annot_new/annoted_by_1to500.json', 'w', encoding='utf-8')\n",
    "save_str = json.dumps(output, ensure_ascii=False, indent=4, separators=(',', ': '))\n",
    "fw.write(save_str)\n",
    "fw.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7847a03-e218-44ef-bb83-5e49ca31b1d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system(\"shutdown\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('jgy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5970cc3159b15dedb1f3a2bddfb758e67a1721376b7397fa1f3df8941c2f173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
