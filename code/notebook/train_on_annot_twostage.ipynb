{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3516121e-224c-4ff6-b113-7f0688056924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import *\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b65642f-6ea7-44bf-bcb5-562aff34f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_constant_schedule_with_warmup, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38dea672-6e55-40bf-8b1a-711039e54a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trainer import BasicTrainer\n",
    "from model import DepParserTwostage\n",
    "from utils import arc_rel_loss_split, uas_las_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364156f-4402-4b34-b357-90cd4683ed92",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c8c13e-2a29-4c4a-a0c0-bc88089e4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_file = '/root/diag_dep/data_new/1to200_0923.json'\n",
    "    plm = 'hfl/chinese-electra-180g-large-discriminator'\n",
    "    num_folds = 5\n",
    "    trn_folds = [0, 1, 2, 3, 4]\n",
    "    random_seed = 42\n",
    "    num_epochs = 15\n",
    "    batch_size = 1  # can not work if batch size != 1\n",
    "    lr = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.2\n",
    "    grad_clip = 1\n",
    "    scheduler = 'linear'\n",
    "    warmup_ratio = 0.12\n",
    "    num_early_stop = 5\n",
    "    max_length = 70\n",
    "    max_turns = 56\n",
    "    num_labels = 40\n",
    "    hidden_size = 200\n",
    "    print_every = 1e9\n",
    "    eval_every = 100\n",
    "    cuda = True\n",
    "    fp16 = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcaccb-18d3-4c60-a99f-c9ba2ac5687b",
   "metadata": {},
   "source": [
    "## Seed and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6fc6cbe6-687d-423a-a727-b81ea81fed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=CFG.random_seed):\n",
    "    np.random.seed(seed%(2**32-1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9ba0a24-f554-4f67-9459-0938a67c58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75bf13-f8bf-4f44-9fd3-022f474f1dae",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72b2700c-7f24-4014-ba65-2f4fbd5362f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dct = {\n",
    "    'root': '根节点',\n",
    "    'sasubj-obj': '同主同宾',\n",
    "    'sasubj': '同主语',\n",
    "    'dfsubj': '不同主语',\n",
    "    'subj': '主语',\n",
    "    'subj-in': '内部主语',\n",
    "    'obj': '宾语',\n",
    "    'pred': '谓语',\n",
    "    'att': '定语',\n",
    "    'adv': '状语',\n",
    "    'cmp': '补语',\n",
    "    'coo': '并列',\n",
    "    'pobj': '介宾',\n",
    "    'iobj': '间宾',\n",
    "    'de': '的',\n",
    "    'adjct': '附加',\n",
    "    'app': '称呼',\n",
    "    'exp': '解释',\n",
    "    'punc': '标点',\n",
    "    'frag': '片段',\n",
    "    'repet': '重复',\n",
    "    # rst\n",
    "    'attr': '归属',\n",
    "    'bckg': '背景',\n",
    "    'cause': '因果',\n",
    "    'comp': '比较',\n",
    "    'cond': '状况',\n",
    "    'cont': '对比',\n",
    "    'elbr': '阐述',\n",
    "    'enbm': '目的',\n",
    "    'eval': '评价',\n",
    "    'expl': '解释-例证',\n",
    "    'joint': '联合',\n",
    "    'manner': '方式',\n",
    "    'rstm': '重申',\n",
    "    'temp': '时序',\n",
    "    'tp-chg': '主题变更',\n",
    "    'prob-sol': '问题-解决',\n",
    "    'qst-ans': '疑问-回答',\n",
    "    'stm-rsp': '陈述-回应',\n",
    "    'req-proc': '需求-处理',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75bf12c6-e493-453b-a3ca-a4eb15b24bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rst_lst = [\n",
    "    'attr','bckg','cause','comp','cond','cont','elbr','enbm','eval','expl','joint',\n",
    "    'manner','rstm','temp','tp-chg','prob-sol','qst-ans','stm-rsp','req-proc',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35cf9c30-727a-489d-97e6-efc33c2bf5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 0, 'sasubj-obj': 1, 'sasubj': 2, 'dfsubj': 3, 'subj': 4, 'subj-in': 5, 'obj': 6, 'pred': 7, 'att': 8, 'adv': 9, 'cmp': 10, 'coo': 11, 'pobj': 12, 'iobj': 13, 'de': 14, 'adjct': 15, 'app': 16, 'exp': 17, 'punc': 18, 'frag': 19, 'repet': 20, 'attr': 21, 'bckg': 22, 'cause': 23, 'comp': 24, 'cond': 25, 'cont': 26, 'elbr': 27, 'enbm': 28, 'eval': 29, 'expl': 30, 'joint': 31, 'manner': 32, 'rstm': 33, 'temp': 34, 'tp-chg': 35, 'prob-sol': 36, 'qst-ans': 37, 'stm-rsp': 38, 'req-proc': 39}\n"
     ]
    }
   ],
   "source": [
    "rel2id = {key:idx for idx, key in enumerate(rel_dct.keys())}\n",
    "print(rel2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0d8e193-ac84-4024-9890-3564fe797318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "add token: [root] 21128\n",
      "add token: [qst] 21129\n",
      "add token: [ans] 138\n",
      "21131\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.plm)\n",
    "print(len(tokenizer))\n",
    " \n",
    "num_added_toks = tokenizer.add_tokens(['[root]', '[qst]', '[aws]'], special_tokens=True)\n",
    "tokenizer.root_token = '[root]'\n",
    "tokenizer.root_token_ids = tokenizer('[root]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.root_token} {tokenizer.root_token_ids}\")\n",
    "\n",
    "tokenizer.root_token = '[qst]'\n",
    "tokenizer.root_token_ids = tokenizer('[qst]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.root_token} {tokenizer.root_token_ids}\")\n",
    "\n",
    "tokenizer.root_token = '[ans]'\n",
    "tokenizer.root_token_ids = tokenizer('[ans]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.root_token} {tokenizer.root_token_ids}\")\n",
    "print(len(tokenizer))\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c037592f-c982-46c3-a294-87c076c7d9ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "626\n",
      "209.725\n"
     ]
    }
   ],
   "source": [
    "with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)[:200]  # have annotated 200 data\n",
    "\n",
    "sent_lst = []\n",
    "max_len, trun_cnt = 0, 0\n",
    "for d in data:\n",
    "    word_lst = []\n",
    "    for item in d['dialog']:\n",
    "        word_lst.extend(item['utterance'].split(' '))\n",
    "    if len(word_lst) > max_len:\n",
    "        max_len = len(word_lst)\n",
    "    if len(word_lst) > 450:\n",
    "        trun_cnt += 1\n",
    "    sent_lst.append(len(word_lst))\n",
    "\n",
    "print(trun_cnt)\n",
    "print(max_len)\n",
    "print(np.mean(sent_lst))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "88f0dfad-12d1-4546-b05e-ad36bffaa720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependency():\n",
    "    def __init__(self, idx, word, head, rel):\n",
    "        self.id = idx\n",
    "        self.word = word\n",
    "        self.tag = '_'\n",
    "        self.head = head\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self):\n",
    "        # example:  1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        values = [str(self.idx), self.word, \"_\", self.tag, \"_\", \"_\", str(self.head), self.rel, \"_\", \"_\"]\n",
    "        return '\\t'.join(values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.id}, {self.word}, {self.tag}, {self.head}, {self.rel})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3154859d-3591-4177-94ef-137822dc4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annoted(data_file):\n",
    "    max_turns = 0\n",
    "    with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)[:200]  # have annotated 200 data\n",
    "        \n",
    "    sample_lst:List[List[Dependency]] = []\n",
    "    \n",
    "    for d in data:\n",
    "        rel_dct = {}\n",
    "        for tripple in d['relationship']:\n",
    "            head, rel, tail = tripple\n",
    "            head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "            tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "            \n",
    "            if rel == 'root' and head_uttr_idx != 0: # ignore root\n",
    "                continue\n",
    "                 \n",
    "            if not rel_dct.get(tail_uttr_idx, None):\n",
    "                rel_dct[tail_uttr_idx] = {tail_word_idx: [head, rel]}\n",
    "            else:\n",
    "                rel_dct[tail_uttr_idx][tail_word_idx] = [head, rel]\n",
    "                \n",
    "        sent_lens_accum = [0]\n",
    "        for i, item in enumerate(d['dialog']):\n",
    "            utterance = item['utterance']\n",
    "            sent_lens_accum.append(sent_lens_accum[i] + len(utterance.split(' ')) + 1)\n",
    "        \n",
    "        one_sample:List[List[Dependency]] = []\n",
    "        for item in d['dialog']:\n",
    "            turn = item['turn']\n",
    "            if turn > max_turns:\n",
    "                max_turns = turn\n",
    "            utterance = item['utterance']\n",
    "            # dep_lst:List[Dependency] = [Dependency(0, '[root]', -1, '_')]\n",
    "            \n",
    "            role = '[ans]' if item['speaker'] == 'A' else '[qst]'\n",
    "            dep_lst = [Dependency(0, role, -1, '_')]\n",
    "            \n",
    "            for word_idx, word in enumerate(utterance.split(' ')):\n",
    "                tail2head = rel_dct.get(turn, {1: [f'{turn}-{word_idx}', 'adjct']})\n",
    "                head, rel = tail2head.get(word_idx + 1, [f'{turn}-{word_idx}', 'adjct'])  # some word annoted missed, padded with last word and 'adjct'\n",
    "                head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "                \n",
    "                if head_uttr_idx == turn:\n",
    "                    dep_lst.append(Dependency(word_idx + 1, word, head_word_idx, rel)) \n",
    "                else:\n",
    "                    dep_lst.append(Dependency(word_idx + 1, word, sent_lens_accum[head_uttr_idx] + head_word_idx, rel))  # add with accumulated length\n",
    "            one_sample.append(dep_lst)\n",
    "         \n",
    "        sample_lst.append(one_sample)\n",
    "    \n",
    "    print(max_turns)\n",
    "    return sample_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d424fc76-bc43-46ca-9852-9823d8f9ad24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data = load_annoted(CFG.data_file)\n",
    "# data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13ac09de-1f8b-4614-aab5-ba4f2bc9596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.inputs, self.offsets, self.heads, self.rels, self.masks = self.read_data()\n",
    "        \n",
    "    def read_data(self):\n",
    "        inputs, offsets = [], []\n",
    "        tags, heads, rels, masks = [], [], [], []\n",
    "        \n",
    "        for deps_lst in load_annoted(self.cfg.data_file):\n",
    "            head_tokens = np.zeros((self.cfg.max_turns, self.cfg.max_length), dtype=np.int64)  # same as root index is 0, constrainting by mask \n",
    "            rel_tokens = np.zeros((self.cfg.max_turns, self.cfg.max_length), dtype=np.int64)\n",
    "            mask_tokens = np.zeros((self.cfg.max_turns, self.cfg.max_length), dtype=np.int64)\n",
    "            \n",
    "            inputs_one = []\n",
    "            sentence_word_ids = []\n",
    "            \n",
    "            for idx, deps in enumerate(deps_lst):\n",
    "                if idx + 1 == self.cfg.max_turns:\n",
    "                    break\n",
    "                seq_len = len(deps)\n",
    "\n",
    "                word_lst = [] \n",
    "                for i, dep in enumerate(deps):\n",
    "                    if i == seq_len or i + 1== self.cfg.max_length:\n",
    "                        break\n",
    "\n",
    "                    word_lst.append(dep.word)\n",
    "\n",
    "                    if int(dep.head) == -1 or int(dep.head) >= self.cfg.max_length:\n",
    "                        head_tokens[idx, i+1] = 0\n",
    "                        mask_tokens[idx, i+1] = 0\n",
    "                    else:\n",
    "                        head_tokens[idx, i+1] = int(dep.head)\n",
    "                        mask_tokens[idx, i+1] = 1\n",
    "\n",
    "                    rel_tokens[idx, i+1] = rel2id.get(dep.rel, 0)\n",
    "\n",
    "                tokenized = tokenizer.encode_plus(word_lst, \n",
    "                                                  padding='max_length', \n",
    "                                                  truncation=True,\n",
    "                                                  max_length=self.cfg.max_length, \n",
    "                                                  return_offsets_mapping=True, \n",
    "                                                  return_tensors='pt',\n",
    "                                                  is_split_into_words=True)\n",
    "                inputs_one.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "                                   \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                                   \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                              })\n",
    "\n",
    "                sentence_word_idx = []\n",
    "                for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "                    if start == 0 and end != 0:\n",
    "                        sentence_word_idx.append(idx)\n",
    "\n",
    "                if len(sentence_word_idx) < self.cfg.max_length - 1:\n",
    "                    sentence_word_idx.extend([0]* (self.cfg.max_length - 1 - len(sentence_word_idx)))\n",
    "                    \n",
    "                sentence_word_ids.append(sentence_word_idx)\n",
    "            \n",
    "            inputs_merge = {\"input_ids\": torch.Tensor(), \"token_type_ids\": torch.Tensor(), \"attention_mask\": torch.Tensor()}\n",
    "            for tokenized in inputs_one:\n",
    "                for key, value in tokenized.items():\n",
    "                    inputs_merge[key] = torch.cat([inputs_merge[key], value.unsqueeze(0)], dim=0)\n",
    "            \n",
    "            # padding\n",
    "            inputs_merge = {key: torch.cat([value, torch.zeros(CFG.max_turns - value.size()[0], CFG.max_length)], dim=0).int() for key, value in inputs_merge.items()}\n",
    "            sentence_word_ids = torch.as_tensor(sentence_word_ids)\n",
    "            sentence_word_ids = torch.cat([sentence_word_ids, torch.zeros(CFG.max_turns - sentence_word_ids.size()[0], CFG.max_length - 1)], dim=0).long()\n",
    "                    \n",
    "            inputs.append(inputs_merge)        \n",
    "            offsets.append(sentence_word_ids)\n",
    "            heads.append(head_tokens)\n",
    "            rels.append(rel_tokens)\n",
    "            masks.append(mask_tokens)\n",
    "                   \n",
    "        return inputs, offsets, heads, rels, masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.offsets[idx], self.heads[idx], self.rels[idx], self.masks[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "303101a6-a774-456a-bb1d-abc879b0622e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56\n"
     ]
    }
   ],
   "source": [
    "dataset = DialogDataset(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bece90b1-a5aa-4f14-bb8c-781d37f89e98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 70])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][0]['input_ids'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8f04c313-91fb-45f0-800b-d100ccb53c19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([56, 69])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e7e4e6d-1f39-4f62-a6f3-e7958311e0a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 70)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b8a8fbe4-5f92-4fb1-8e23-619b9fd0e4bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 70)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8b79cd5f-1921-4df0-b028-53b368985c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 70)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0][4].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "48c3e47c-bd75-4675-b28c-c3afc53315c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = DataLoader(dataset, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f25199fc-0185-411f-918f-fc37c98dbdde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 56, 70])\n"
     ]
    }
   ],
   "source": [
    "for item in data_iter:\n",
    "    print(item[0]['input_ids'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "571d8feb-a553-4f93-a76b-70a5869af34a",
   "metadata": {},
   "source": [
    "## Tranining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b84fd604-50a5-436a-ac18-3242bb6c489f",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=CFG.num_folds, shuffle=True, random_state=CFG.random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e60957b5-606b-440b-8b5e-77d9c609b337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "160/40\n",
      "--------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_65358/2289708677.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mva_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mva_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDepParserTwostage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCFG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     optim = AdamW(model.parameters(), \n",
      "\u001b[0;32m~/diag_dep/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cfg)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_rel_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_token_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m         raise ValueError(\n\u001b[1;32m    448\u001b[0m             \u001b[0;34mf\"Unrecognized configuration class {config.__class__} for this kind of AutoModel: {cls.__name__}.\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_sharded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1796\u001b[0m                 \u001b[0;31m# Time to load the checkpoint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1797\u001b[0;31m                 \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1798\u001b[0m             \u001b[0;31m# set dtype to instantiate the model under:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1799\u001b[0m             \u001b[0;31m# 1. If torch_dtype is not None, we use that dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file)\u001b[0m\n\u001b[1;32m    347\u001b[0m     \"\"\"\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    606\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 608\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    609\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.8/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    792\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    793\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 794\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    796\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    if fold not in CFG.trn_folds:\n",
    "        continue\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'{len(train_ids)}/{len(val_ids)}')\n",
    "    print('--------------------------------')\n",
    "\n",
    "    if CFG.cuda and torch.cuda.is_available:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    random.shuffle(train_ids)\n",
    "    random.shuffle(val_ids)\n",
    "\n",
    "    tr_dataset = Subset(dataset, train_ids)\n",
    "    va_dataset = Subset(dataset, val_ids)\n",
    "    \n",
    "    tr_iter = DataLoader(tr_dataset, batch_size=CFG.batch_size)\n",
    "    va_iter = DataLoader(va_dataset, batch_size=CFG.batch_size)\n",
    "    \n",
    "    model = DepParserTwostage(CFG)\n",
    "    \n",
    "    optim = AdamW(model.parameters(), \n",
    "                      lr=CFG.lr,\n",
    "                      weight_decay=CFG.weight_decay\n",
    "                      )\n",
    "\n",
    "    training_step = int(CFG.num_epochs * (len(train_ids) / CFG.batch_size))\n",
    "    warmup_step = int(CFG.warmup_ratio * training_step)  \n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optim, \n",
    "                                                        num_warmup_steps=warmup_step, \n",
    "                                                        num_training_steps=training_step)\n",
    "\n",
    "    trainer = BasicTrainer(optim=optim, \n",
    "                        lr_scheduler=lr_scheduler,\n",
    "                        trainset_size=len(train_ids), \n",
    "                        loss_fn=arc_rel_loss_split, \n",
    "                        metrics_fn=uas_las_split, \n",
    "                        config=CFG)\n",
    "    \n",
    "    best_res, best_state_dict = trainer.train(model=model, train_iter=tr_iter, val_iter=va_iter)\n",
    "    print(best_res)\n",
    "    with open(\"/root/autodl-tmp/diag_dep/k-fold/res.txt\", 'a+') as f:\n",
    "        f.write(f'{fold}\\t {str(best_res)}\\n')\n",
    "    \n",
    "    torch.save(best_state_dict, f\"/root/autodl-tmp/diag_dep/k-fold/{fold}/model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c396a148-e2cf-4862-9d12-4cb96396bcc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system('shutdown')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('jgy')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5970cc3159b15dedb1f3a2bddfb758e67a1721376b7397fa1f3df8941c2f173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
