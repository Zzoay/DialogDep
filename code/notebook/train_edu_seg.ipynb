{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2b8d214-62f2-4fb3-b95d-fe9f51392251",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import Counter\n",
    "from typing import *\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6fda167-e4d0-4040-8a20-fbd6d5808b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AdamW, get_constant_schedule_with_warmup, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "239dd15c-ae6a-4c3e-b468-467ede572ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import to_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e58cb94-fd16-42b8-b355-97db72655bf7",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "953e2609-69e4-4d30-9ba3-b9d6a4995d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    np.random.seed(seed%(2**32-1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddd9d8a-3f5e-4c3f-91d7-9f4fefe3f855",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b867daef-9c1a-4bc5-b2eb-446ffe1c2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    file = 'data/train_proc.json'\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)   \n",
    "        \n",
    "    punct_lst = [\"，\", \".\", \"。\", \"!\", \"?\", \"~\", \"...\", \"......\"]\n",
    "    single_max_len = 128\n",
    "    concat_max_len = 64\n",
    "    long_len = 12\n",
    "    long_keep_prob = 0.6\n",
    "    concat_prob = 0.5\n",
    "    punct_probs = {\n",
    "        \"\": 0.25,\n",
    "        \"，\": 0.55,\n",
    "        \"。\": 0.05,\n",
    "        \"!\": 0.05,\n",
    "        \"?\": 0.05,\n",
    "        \"~\": 0.05,\n",
    "    }\n",
    "    special_tokens = {\n",
    "        \"Q\": \"[qst]\",\n",
    "        \"A\": \"[ans]\",\n",
    "    }\n",
    "    \n",
    "    sample_lst = []\n",
    "    for d in data:\n",
    "        concat = {'Q': {\n",
    "            'word_lst': [], 'split_ids': []\n",
    "        }, \n",
    "                  'A': {\n",
    "            'word_lst': [], 'split_ids': []\n",
    "        }}\n",
    "\n",
    "        for turn, dialog in enumerate(d.get('dialog')):\n",
    "            speaker, utterance = dialog.get('speaker'), dialog.get('utterance')\n",
    "            word_lst = utterance.split(' ')\n",
    "            if len(word_lst) >= single_max_len:\n",
    "                continue    \n",
    "        \n",
    "            if len(word_lst) >= long_len:\n",
    "                if random.random() < long_keep_prob:\n",
    "                    split_lst = []\n",
    "                    for idx, word in enumerate(word_lst[:-1]):\n",
    "                        if word_lst[idx+1] not in punct_lst and random.random() < punct_probs.get(word, 0.0):\n",
    "                            split_lst.append(idx + 1)\n",
    "                    if len(split_lst) == 0 or split_lst[-1] != len(word_lst):\n",
    "                        split_lst.append(len(word_lst))\n",
    "                    sample_lst.append({'word_lst': [special_tokens[speaker]] + word_lst, 'split_ids': split_lst})\n",
    "                if len(concat[speaker]['word_lst']) != 0:\n",
    "                    sample = {'word_lst': [special_tokens[speaker]] + concat[speaker]['word_lst'], 'split_ids': concat[speaker]['split_ids']}\n",
    "                    sample_lst.append(sample)\n",
    "                    concat[speaker] = {'word_lst': [], 'split_ids': []}\n",
    "                continue\n",
    "\n",
    "            if word_lst[-1] not in punct_lst:\n",
    "                punct = random.choices(list(punct_probs.keys()), weights=punct_probs.values(), k=1)\n",
    "                if punct != ['']:\n",
    "                    word_lst += punct\n",
    "            \n",
    "            all_punct = True\n",
    "            for word in word_lst:\n",
    "                if word not in punct_lst:\n",
    "                    all_punct = False\n",
    "                    break\n",
    "\n",
    "            if all_punct:\n",
    "                # pop last split index\n",
    "                if len(concat[speaker]['split_ids']) != 0:\n",
    "                    concat[speaker]['split_ids'].pop()\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            concat[speaker]['word_lst'].extend(word_lst)\n",
    "            concat[speaker]['split_ids'].append(len(concat[speaker]['word_lst']))\n",
    "            \n",
    "            # add sample\n",
    "            if len(concat[speaker]['word_lst']) >= concat_max_len - 1 or random.random() > concat_prob:\n",
    "                if concat[speaker]['word_lst'][-1] == '，':  # '，' is seldom appear at the end of a sentence\n",
    "                    concat[speaker]['word_lst'] = concat[speaker]['word_lst'][:-1]\n",
    "                    concat[speaker]['split_ids'][-1] -= 1\n",
    "                sample = {'word_lst': [special_tokens[speaker]] + concat[speaker]['word_lst'], 'split_ids': concat[speaker]['split_ids']}\n",
    "                sample_lst.append(sample)\n",
    "                concat[speaker] = {'word_lst': [], 'split_ids': []}  \n",
    "                     \n",
    "    print(len(sample_lst))\n",
    "    \n",
    "    ret = []\n",
    "    for sample in sample_lst:\n",
    "        if len(sample['word_lst']) <= concat_max_len:\n",
    "            ret.append(sample)\n",
    "    \n",
    "    sample_lst = ret\n",
    "    del ret\n",
    "    print(len(sample_lst))\n",
    "    \n",
    "    with open('data/seg_with_train.json', 'w', encoding='utf-8') as f:\n",
    "        save_str = json.dumps(sample_lst, ensure_ascii=False, indent=4, separators=(',', ': '))\n",
    "        f.write(save_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeb87577-430a-4539-bcb9-c813a5d037be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "127218\n",
      "126800\n"
     ]
    }
   ],
   "source": [
    "prepare_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dc87b7-cdd5-4e07-85fc-277dd42200c0",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ebf03d7d-52c4-4fe3-a1f8-b5956994078e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_file = 'data/seg_with_train.json'\n",
    "    # data_file = 'data_testset/1to500_1012.json'\n",
    "    plm = 'hfl/chinese-electra-180g-large-discriminator'\n",
    "    num_folds = 5\n",
    "    trn_folds = [0]  # only one fold, as splitting train/val randomly\n",
    "    # trn_folds = [0, 1, 2, 3, 4]\n",
    "    random_seed = 42\n",
    "    num_epochs = 3\n",
    "    batch_size = 48\n",
    "    max_length = 128\n",
    "    # batch_size = 32\n",
    "    # max_length = 160\n",
    "    num_labels = 2\n",
    "    lr = 2e-5\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.2\n",
    "    grad_clip = 1\n",
    "    scheduler = 'linear'\n",
    "    warmup_ratio = 0.1\n",
    "    num_early_stop = 5\n",
    "    hidden_size = 400\n",
    "    print_every = 500  \n",
    "    eval_every = 1000\n",
    "    # print_every = 200  \n",
    "    # eval_every = 300\n",
    "    cuda = True\n",
    "    fp16 = True\n",
    "    debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e3de54-45b1-4593-b053-6c4a4c6f7b8a",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "06814b09-4b0c-430e-af1d-9e1f0f6c6cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "    \n",
    "# data = data[:500]  # 500 data have been annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77de8e84-beaf-491c-b982-28541b96ad4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "add token: [root] 21128\n",
      "add token: [qst] 21129\n",
      "add token: [ans] 21130\n",
      "21131\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.plm)\n",
    "print(len(tokenizer))\n",
    " \n",
    "num_added_toks = tokenizer.add_tokens(['[root]', '[qst]', '[ans]'], special_tokens=True)\n",
    "tokenizer.root_token = '[root]'\n",
    "tokenizer.root_token_ids = tokenizer('[root]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.root_token} {tokenizer.root_token_ids}\")\n",
    "\n",
    "tokenizer.qst_token = '[qst]'\n",
    "tokenizer.qst_token_ids = tokenizer('[qst]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.qst_token} {tokenizer.qst_token_ids}\")\n",
    "\n",
    "tokenizer.ans_token = '[ans]'\n",
    "tokenizer.ans_token_ids = tokenizer('[ans]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.ans_token} {tokenizer.ans_token_ids}\")\n",
    "print(len(tokenizer))\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fea18026-f2e4-4c94-803d-238656fc01b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EduDataset(Dataset):\n",
    "    def __init__(self, cfg, data):\n",
    "        self.cfg = cfg\n",
    "        self.data = data\n",
    "        self.inputs, self.offsets, self.tags = self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        inputs, offsets, tags = [], [], []\n",
    "            \n",
    "        for sample in tqdm(self.data):\n",
    "            word_lst, split_ids = sample['word_lst'], sample['split_ids']\n",
    "            tokenized = self.cfg.tokenizer.encode_plus(word_lst, \n",
    "                                                       padding='max_length', \n",
    "                                                       truncation=True,\n",
    "                                                       max_length=self.cfg.max_length + 2,  # reserved for cls and sep\n",
    "                                                       return_offsets_mapping=True, \n",
    "                                                       return_tensors='pt',\n",
    "                                                       is_split_into_words=True)\n",
    "            \n",
    "            inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "                           \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                           \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                          })\n",
    "            \n",
    "            sentence_word_idx = []\n",
    "            for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "                if start == 0 and end != 0:\n",
    "                    sentence_word_idx.append(idx)\n",
    "            if len(sentence_word_idx) < self.cfg.max_length - 1:\n",
    "                sentence_word_idx.extend([0]* (self.cfg.max_length - len(sentence_word_idx)))\n",
    "            offsets.append(torch.as_tensor(sentence_word_idx))\n",
    "            \n",
    "            # ignore cls for convenience\n",
    "            tag = torch.full(size=(CFG.max_length, ), fill_value=-1, dtype=torch.long)\n",
    "            tag[0:split_ids[-1]+1] = 0\n",
    "            tag[split_ids[:-1]] = 1\n",
    "            \n",
    "            tags.append(tag)  \n",
    "            \n",
    "        return inputs, offsets, tags\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.offsets[idx], self.tags[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7245125d-4d83-4b1d-9f41-01e36cdddde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = tokenizer.encode_plus(data[0]['word_lst'], \n",
    "                                   padding='max_length', \n",
    "                                   truncation=True,\n",
    "                                   max_length=CFG.max_length + 2,  # reserved for cls and sep\n",
    "                                   return_offsets_mapping=True, \n",
    "                                   return_tensors='pt',\n",
    "                                   is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6389b581-44e8-4bcb-97a0-649ba7ff4e85",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101, 21130,   779,   779,  8024,  3300,   784,   720,  7309,  7579,\n",
       "          2769,  1377,   809,  2376,  2644,  1905,  4415,  2772,  6237,  1104,\n",
       "          4638,  1450,   136,   102,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2d36bc6b-f051-4bca-9de6-8209d322b8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Dependency():\n",
    "#     def __init__(self, idx, word, head, rel):\n",
    "#         self.id = idx\n",
    "#         self.word = word\n",
    "#         self.tag = '_'\n",
    "#         self.head = head\n",
    "#         self.rel = rel\n",
    "\n",
    "#     def __str__(self):\n",
    "#         # example:  1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "#         values = [str(self.idx), self.word, \"_\", self.tag, \"_\", \"_\", str(self.head), self.rel, \"_\", \"_\"]\n",
    "#         return '\\t'.join(values)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return f\"({self.word}, {self.tag}, {self.head}, {self.rel})\"\n",
    "\n",
    "# rst_dct = {\n",
    "#     'attr': '归属',\n",
    "#     'bckg': '背景',\n",
    "#     'cause': '因果',\n",
    "#     'comp': '比较',\n",
    "#     'cond': '状况',\n",
    "#     'cont': '对比',\n",
    "#     'elbr': '阐述',\n",
    "#     'enbm': '目的',\n",
    "#     'eval': '评价',\n",
    "#     'expl': '解释-例证',\n",
    "#     'joint': '联合',\n",
    "#     'manner': '方式',\n",
    "#     'rstm': '重申',\n",
    "#     'temp': '时序',\n",
    "#     'tp-chg': '主题变更',\n",
    "#     'prob-sol': '问题-解决',\n",
    "#     'qst-ans': '疑问-回答',\n",
    "#     'stm-rsp': '陈述-回应',\n",
    "#     'req-proc': '需求-处理',\n",
    "# }\n",
    "\n",
    "# rst_lst = [x for x in rst_dct.keys()]\n",
    "# print(rst_lst)\n",
    "\n",
    "# def data_gen(data):\n",
    "#     special_tokens = {\n",
    "#         \"Q\": \"[qst]\",\n",
    "#         \"A\": \"[ans]\",\n",
    "#     }    \n",
    "\n",
    "#     for i, d in enumerate(data):\n",
    "#         rel_dct = {}\n",
    "#         for tripple in d['relationship']:\n",
    "#             head, rel, tail = tripple\n",
    "#             head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "#             tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "#             if head_uttr_idx != tail_uttr_idx:\n",
    "#                 continue\n",
    "\n",
    "#             if not rel_dct.get(head_uttr_idx, None):\n",
    "#                 rel_dct[head_uttr_idx] = {tail_word_idx: [head_word_idx, rel]}\n",
    "#             else:\n",
    "#                 rel_dct[head_uttr_idx][tail_word_idx] = [head_word_idx, rel]\n",
    "\n",
    "\n",
    "#         for item in d['dialog']:\n",
    "#             turn = item['turn']\n",
    "#             speaker = item['speaker']\n",
    "#             utterance = item['utterance']\n",
    "#             dep_lst:List[Dependency] = []\n",
    "\n",
    "#             edus = [[-1, -1]]  # 0 for root\n",
    "#             inner_rels, inter_rels = [], []\n",
    "#             for word_idx, word in enumerate(utterance.split(' ')):\n",
    "#                 head_word_idx, rel = rel_dct[turn].get(word_idx + 1, [word_idx, 'adjct'])  # some word annoted missed, padded with last word and 'adjct'\n",
    "\n",
    "#                 if rel in rst_lst:\n",
    "#                     inter_rels.append(rel)\n",
    "#                     edus.append([word_idx + 1, word_idx + 1])\n",
    "#                     continue\n",
    "\n",
    "#                 expand, include = False, False\n",
    "#                 for edu in edus:\n",
    "#                     if edu[0] <= head_word_idx <= edu[1]:\n",
    "#                         edu[0] = min(edu[0], word_idx + 1)\n",
    "#                         edu[1] = max(edu[1], word_idx + 1)\n",
    "#                         expand = True\n",
    "#                         break\n",
    "#                     elif edu[0] <= word_idx + 1 <= edu[1] and head_word_idx != 0:\n",
    "#                         edu[0] = min(edu[0], head_word_idx)\n",
    "#                         edu[1] = max(edu[1], head_word_idx)\n",
    "#                         include = True\n",
    "#                         break\n",
    "\n",
    "#                 if not expand and not include:\n",
    "#                     if head_word_idx == 0:  # ignore root\n",
    "#                         edus.append([word_idx + 1, word_idx + 1])\n",
    "#                     else:\n",
    "#                         # max with 1 to ignore root index\n",
    "#                         edus.append([max(min(word_idx + 1, head_word_idx), 1), max(word_idx + 1, head_word_idx)])\n",
    "#                 inner_rels.append(rel)\n",
    "\n",
    "#                 dep_lst.append(Dependency(word_idx + 1, word, head_word_idx, rel))  # start from 1\n",
    "\n",
    "#             finals = [edus[1]]\n",
    "#             if len(edus) >= 2:\n",
    "#                 for edu in edus[2:]:\n",
    "#                     if finals[-1][0] <= edu[0] <= edu[1] <= finals[-1][1]:\n",
    "#                         continue\n",
    "#                     finals.append(edu)\n",
    "\n",
    "#             split_ids = [x[1] for x in finals]\n",
    "\n",
    "#             yield [special_tokens[speaker]] + utterance.split(' '), split_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0dfe374c-f555-4744-a686-a470c1d8f478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EduDataset(Dataset):\n",
    "#     def __init__(self, cfg, data):\n",
    "#         self.cfg = cfg\n",
    "#         self.data = data\n",
    "#         self.inputs, self.offsets, self.tags = self.load_data()\n",
    "    \n",
    "#     def load_data(self):\n",
    "#         inputs, offsets, tags = [], [], []\n",
    "            \n",
    "#         for word_lst, split_ids in tqdm(data_gen(self.data)):\n",
    "#             tokenized = self.cfg.tokenizer.encode_plus(word_lst, \n",
    "#                                                        padding='max_length', \n",
    "#                                                        truncation=True,\n",
    "#                                                        max_length=self.cfg.max_length + 2,  # reserved for cls and sep\n",
    "#                                                        return_offsets_mapping=True, \n",
    "#                                                        return_tensors='pt',\n",
    "#                                                        is_split_into_words=True)\n",
    "            \n",
    "#             inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "#                            \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "#                            \"attention_mask\": tokenized['attention_mask'][0]\n",
    "#                           })\n",
    "            \n",
    "#             sentence_word_idx = []\n",
    "#             for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "#                 if start == 0 and end != 0:\n",
    "#                     sentence_word_idx.append(idx)\n",
    "#             if len(sentence_word_idx) < self.cfg.max_length - 1:\n",
    "#                 sentence_word_idx.extend([0]* (self.cfg.max_length - len(sentence_word_idx)))\n",
    "#             offsets.append(torch.as_tensor(sentence_word_idx))\n",
    "            \n",
    "#             # ignore cls for convenience\n",
    "#             tag = torch.full(size=(CFG.max_length, ), fill_value=-1, dtype=torch.long)\n",
    "#             tag[0:split_ids[-1]] = 0\n",
    "#             tag[split_ids[:-1]] = 1\n",
    "            \n",
    "#             tags.append(tag)  \n",
    "            \n",
    "#         return inputs, offsets, tags\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.inputs[idx], self.offsets[idx], self.tags[idx]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b13ab20a-5c0d-4eb8-bc59-ac0774de0550",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    data = data[:3000]\n",
    "    CFG.print_every = 20\n",
    "    CFG.eval_every = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "de12d4ca-ba30-4008-b08c-cdb9a2785b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 126800/126800 [04:19<00:00, 488.97it/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = EduDataset(CFG, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "877bf214-9948-42f6-8a42-f185233204b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.encoder = AutoModel.from_pretrained(cfg.plm)\n",
    "        self.encoder.resize_token_embeddings(len(cfg.tokenizer))\n",
    "        \n",
    "        # self.gru = nn.GRU(self.encoder.config.hidden_size, self.encoder.config.hidden_size//2, bidirectional=True, num_layers=1, batch_first=True)\n",
    "        \n",
    "        self.mlp = nn.Linear(self.encoder.config.hidden_size, cfg.num_labels)\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "        \n",
    "    def feat(self, inputs, offsets):\n",
    "        length = torch.sum(inputs[\"attention_mask\"], dim=-1) - 2\n",
    "        \n",
    "        feats, *_ = self.encoder(**inputs, return_dict=False)   # batch_size, seq_len (tokenized), plm_hidden_size\n",
    "           \n",
    "        # remove [CLS] [SEP]\n",
    "        cls_feat = feats[:, :1]\n",
    "        char_feat = torch.narrow(feats, 1, 1, feats.size(1) - 2)\n",
    "        return cls_feat, char_feat, length\n",
    "        \n",
    "    def forward(self, inputs, offsets, tags=None):\n",
    "        cls_feat, char_feat, char_len = self.feat(inputs, offsets)\n",
    "        \n",
    "        word_idx = offsets.unsqueeze(-1).expand(-1, -1, char_feat.shape[-1])  # expand to the size of char feat\n",
    "        word_feat = torch.gather(char_feat, dim=1, index=word_idx)  # embeddings of first char in each word\n",
    "        \n",
    "        # feats, _ = self.gru(self.dropout(word_feat))\n",
    "        \n",
    "        feats = self.dropout(word_feat)\n",
    "        logits = self.mlp(feats)\n",
    "        \n",
    "        if tags is not None:\n",
    "            loss = nn.CrossEntropyLoss(reduction='mean', ignore_index=-1)(logits.view(-1, logits.size(-1)), tags.long().view(-1))\n",
    "            return torch.softmax(logits, dim=-1), loss\n",
    "\n",
    "        return torch.softmax(logits, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b81d504a-292f-48b0-b025-82ede6c456f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = SegModel(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4dc4933c-2492-4850-b6c5-5d03d73ad602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_iter = DataLoader(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4daefe68-26dc-44fd-a7e8-ea99de75b3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i, batch in enumerate(data_iter):\n",
    "#     print(batch[0])\n",
    "#     print(tokenizer.decode(batch[0]['input_ids'][0]))\n",
    "#     print(batch[2])\n",
    "    \n",
    "#     if i == 1:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ee0e4ab-659d-4984-b899-738bd2c7578b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metrics_fn(predictions, labels):\n",
    "    predictions = predictions.view(-1, predictions.size(-1))\n",
    "    labels = labels.view(-1)\n",
    "    \n",
    "    pred_detected = (predictions[:, 1] > 0.5)\n",
    "    label_detected = (labels == 1)\n",
    "    co = pred_detected * (pred_detected == label_detected)\n",
    "    \n",
    "    if sum(pred_detected) == 0:\n",
    "        precision = 0\n",
    "    else:\n",
    "        precision = sum(co) / sum(pred_detected)\n",
    "    recall = sum(co) / sum(label_detected)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "\n",
    "    return {\"f1\": f1.item()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99699876-1a22-4701-bf1b-dd0715af3984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics_fn(predictions=logits, labels=tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bc35c958-26c7-4bf8-8d25-11c614400489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for inputs, offsets, tags in data_iter:\n",
    "#     logits, loss = model(inputs, offsets, tags)\n",
    "    \n",
    "#     metrics = metrics_fn(predictions=logits, labels=tags)\n",
    "#     print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bb998a81-6fa7-44db-8be7-bd332f96cb11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer():\n",
    "    def __init__(self, \n",
    "                 optim,\n",
    "                 lr_scheduler,\n",
    "                 trainset_size,\n",
    "                 metrics_fn: Callable, \n",
    "                 config: Dict) -> None:\n",
    "\n",
    "        self.optim = optim\n",
    "        self.optim_schedule = lr_scheduler\n",
    "        \n",
    "        self.metrics_fn = metrics_fn\n",
    "        \n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def train(self, \n",
    "              model: nn.Module, \n",
    "              train_iter: DataLoader, \n",
    "              val_iter: DataLoader):\n",
    "        model.train()\n",
    "        if self.config.cuda and torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            pass\n",
    "        \n",
    "        best_res = [0, 0, 0]\n",
    "        early_stop_cnt = 0\n",
    "        best_state_dict = None\n",
    "        step = 0\n",
    "        for epoch in tqdm(range(self.config.num_epochs)):\n",
    "            for batch in train_iter:\n",
    "                inputs, offsets, tags = batch\n",
    "                \n",
    "                if self.config.cuda and torch.cuda.is_available():\n",
    "                    inputs_cuda = {}\n",
    "                    for key,value in inputs.items():\n",
    "                        inputs_cuda[key] = value.cuda()\n",
    "#                         inputs_cuda[key] = value\n",
    "                    inputs = inputs_cuda\n",
    "                    offsets, tags = to_cuda(data=(offsets, tags))\n",
    "                \n",
    "                logits, loss = model(inputs, offsets, tags)\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                if self.config.cuda and self.config.fp16:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.optim)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_norm=self.config.grad_clip)\n",
    "\n",
    "                if self.config.fp16:\n",
    "                    self.scaler.step(self.optim)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optim.step()\n",
    "                self.optim_schedule.step()\n",
    "\n",
    "                metrics = self.metrics_fn(predictions=logits, labels=tags)\n",
    "\n",
    "                if (step) % self.config.print_every == 0:\n",
    "                    print(f\"--epoch {epoch}, step {step}, loss {loss}\")\n",
    "                    print(f\"  {metrics}\")\n",
    "\n",
    "                if val_iter and (step + 1) % self.config.eval_every == 0:\n",
    "                    avg_loss, metrics = self.eval(model, val_iter)\n",
    "                    res = [avg_loss, metrics['f1']]\n",
    "                    if metrics['f1'] > best_res[1]:  # f1\n",
    "                        best_res = res\n",
    "                        best_state_dict = model.state_dict()\n",
    "                        early_stop_cnt = 0\n",
    "                    else:\n",
    "                        early_stop_cnt += 1\n",
    "                    print(\"--Best Evaluation: \")\n",
    "                    print(\"-Loss: {}  F1: {} \\n\".format(*best_res))\n",
    "                    # back to train mode\n",
    "                    model.train()\n",
    "                \n",
    "                if early_stop_cnt >= self.config.num_early_stop:\n",
    "                    print(\"--early stopping, training finished.\")\n",
    "                    return best_res, best_state_dict\n",
    "\n",
    "                step += 1\n",
    "        print(\"--training finished.\")\n",
    "        return best_res, best_state_dict\n",
    "\n",
    "    # eval func\n",
    "    def eval(self, model: nn.Module, eval_iter: DataLoader, save_file: str = \"\", save_title: str = \"\"):\n",
    "        model.eval()\n",
    "\n",
    "        avg_loss, step = 0.0, 0\n",
    "        logits_whole, tags_whole = torch.Tensor(), torch.Tensor()\n",
    "        for step, batch in enumerate(eval_iter):\n",
    "            inputs, offsets, tags = batch\n",
    "                \n",
    "            if self.config.cuda and torch.cuda.is_available():\n",
    "                inputs_cuda = {}\n",
    "                for key,value in inputs.items():\n",
    "                    inputs_cuda[key] = value.cuda()\n",
    "                inputs = inputs_cuda\n",
    "                offsets, tags = to_cuda(data=(offsets, tags))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logits, loss = model(inputs, offsets, tags)\n",
    "                \n",
    "            logits_whole = torch.cat([logits_whole, logits.cpu()], dim=0)\n",
    "            tags_whole = torch.cat([tags_whole, tags.cpu()], dim=0)\n",
    "\n",
    "            avg_loss += loss * len(tags)  # times the batch size of data\n",
    "\n",
    "        metrics = self.metrics_fn(predictions=logits_whole, labels=tags_whole)\n",
    "        \n",
    "        avg_loss /= len(eval_iter.dataset)\n",
    "        print(\"--Evaluation:\")\n",
    "        print(\"-Loss: {}  F1: {} \\n\".format(avg_loss, metrics['f1']))\n",
    "\n",
    "        if save_file != \"\":\n",
    "            results = [save_title, avg_loss.item(), metrics['f1']]  # type: ignore\n",
    "            results = [str(x) for x in results]\n",
    "            with open(save_file, \"a+\") as f:\n",
    "                f.write(\",\".join(results) + \"\\n\")  # type: ignore\n",
    "\n",
    "        return avg_loss.item(), metrics   # type: ignore\n",
    "    \n",
    "    def save_results(self, save_file, save_title, results):\n",
    "        saves = [save_title] + results\n",
    "        saves = [str(x) for x in saves]\n",
    "        with open(save_file, \"a+\") as f:\n",
    "            f.write(\",\".join(saves) + \"\\n\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada8c919-34ca-47be-b931-a847720917ac",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd070eaa-a0ff-42e9-b2b4-612255dbc812",
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=CFG.num_folds, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d1cdea55-2730-47d1-b06a-2a1ff7c024a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 0\n",
      "101440/25360\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-large-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--epoch 0, step 0, loss 0.8980361819267273\n",
      "  {'f1': 0.010603728704154491}\n",
      "--epoch 0, step 500, loss 0.05390365794301033\n",
      "  {'f1': 0.8311688303947449}\n",
      "--Evaluation:\n",
      "-Loss: 0.04614398628473282  F1: 0.7758190631866455 \n",
      "\n",
      "--Best Evaluation: \n",
      "-Loss: 0.04614398628473282  F1: 0.7758190631866455 \n",
      "\n",
      "--epoch 0, step 1000, loss 0.05576056241989136\n",
      "  {'f1': 0.8253968954086304}\n",
      "--epoch 0, step 1500, loss 0.052981190383434296\n",
      "  {'f1': 0.7241379618644714}\n",
      "--Evaluation:\n",
      "-Loss: 0.04305392503738403  F1: 0.7697447538375854 \n",
      "\n",
      "--Best Evaluation: \n",
      "-Loss: 0.04614398628473282  F1: 0.7758190631866455 \n",
      "\n",
      "--epoch 0, step 2000, loss 0.03807033598423004\n",
      "  {'f1': 0.8405797481536865}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [36:28<1:12:56, 2188.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--epoch 1, step 2500, loss 0.033386681228876114\n",
      "  {'f1': 0.8399999737739563}\n",
      "--Evaluation:\n",
      "-Loss: 0.04917750880122185  F1: 0.7673178911209106 \n",
      "\n",
      "--Best Evaluation: \n",
      "-Loss: 0.04614398628473282  F1: 0.7758190631866455 \n",
      "\n",
      "--epoch 1, step 3000, loss 0.05689133703708649\n",
      "  {'f1': 0.8108108043670654}\n",
      "--epoch 1, step 3500, loss 0.030760589987039566\n",
      "  {'f1': 0.8888888955116272}\n",
      "--Evaluation:\n",
      "-Loss: 0.04266449436545372  F1: 0.7940782904624939 \n",
      "\n",
      "--Best Evaluation: \n",
      "-Loss: 0.04266449436545372  F1: 0.7940782904624939 \n",
      "\n",
      "--epoch 1, step 4000, loss 0.04126041382551193\n",
      "  {'f1': 0.8070175051689148}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [1:12:13<36:02, 2162.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--epoch 2, step 4500, loss 0.04014455899596214\n",
      "  {'f1': 0.7894737124443054}\n",
      "--Evaluation:\n",
      "-Loss: 0.04350907728075981  F1: 0.7914131879806519 \n",
      "\n",
      "--Best Evaluation: \n",
      "-Loss: 0.04266449436545372  F1: 0.7940782904624939 \n",
      "\n",
      "--epoch 2, step 5000, loss 0.0398637130856514\n",
      "  {'f1': 0.7999999523162842}\n",
      "--epoch 2, step 5500, loss 0.03099857084453106\n",
      "  {'f1': 0.8125}\n",
      "--Evaluation:\n",
      "-Loss: 0.04599112644791603  F1: 0.7894276976585388 \n",
      "\n",
      "--Best Evaluation: \n",
      "-Loss: 0.04266449436545372  F1: 0.7940782904624939 \n",
      "\n",
      "--epoch 2, step 6000, loss 0.03410981595516205\n",
      "  {'f1': 0.8571429252624512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [1:47:31<00:00, 2150.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--training finished.\n",
      "[0.04266449436545372, 0.7940782904624939]\n",
      "FOLD 1\n",
      "101440/25360\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "for fold, (train_ids, val_ids) in enumerate(kfold.split(dataset)):\n",
    "    print(f'FOLD {fold}')\n",
    "    print(f'{len(train_ids)}/{len(val_ids)}')\n",
    "    print('--------------------------------')\n",
    "    \n",
    "    if fold not in CFG.trn_folds:\n",
    "        break\n",
    "\n",
    "    if CFG.cuda and torch.cuda.is_available:\n",
    "        torch.cuda.empty_cache()\n",
    " \n",
    "    random.shuffle(train_ids)\n",
    "    random.shuffle(val_ids)\n",
    "\n",
    "    tr_dataset = Subset(dataset, train_ids)\n",
    "    va_dataset = Subset(dataset, val_ids)\n",
    "    \n",
    "    tr_iter = DataLoader(tr_dataset, batch_size=CFG.batch_size)\n",
    "    va_iter = DataLoader(va_dataset, batch_size=CFG.batch_size)\n",
    "    \n",
    "    model = SegModel(CFG)\n",
    "    \n",
    "    optim = AdamW(model.parameters(), \n",
    "                      lr=CFG.lr,\n",
    "                      weight_decay=CFG.weight_decay\n",
    "                      )\n",
    "\n",
    "    training_step = int(CFG.num_epochs * (len(train_ids) / CFG.batch_size))\n",
    "    warmup_step = int(CFG.warmup_ratio * training_step)  \n",
    "    lr_scheduler = get_linear_schedule_with_warmup(optimizer=optim, \n",
    "                                                   num_warmup_steps=warmup_step, \n",
    "                                                   num_training_steps=training_step)\n",
    "\n",
    "    trainer = MyTrainer(optim=optim, \n",
    "                        lr_scheduler=lr_scheduler,\n",
    "                        trainset_size=len(train_ids), \n",
    "                        metrics_fn=metrics_fn, \n",
    "                        config=CFG)\n",
    "    \n",
    "    best_res, best_state_dict = trainer.train(model=model, train_iter=tr_iter, val_iter=va_iter)\n",
    "    print(best_res)\n",
    "    with open(\"/root/autodl-tmp/diag_dep/edu_seg/res.txt\", 'a+') as f:\n",
    "        f.write(f'{fold}\\t {str(best_res)}\\n')\n",
    "    \n",
    "    torch.save(best_state_dict, f\"/root/autodl-tmp/diag_dep/edu_seg/{fold}/model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978cf2fd-d7f7-4178-a6d6-8db7cd0083d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('shutdown')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
