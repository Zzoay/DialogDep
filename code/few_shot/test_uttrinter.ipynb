{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3516121e-224c-4ff6-b113-7f0688056924",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from typing import *\n",
    "import random\n",
    "import json\n",
    "import logging\n",
    "import datetime\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b65642f-6ea7-44bf-bcb5-562aff34f852",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41222b60-114d-4172-9eb6-014125a7b844",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformers.logging.set_verbosity_error() # only report errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38dea672-6e55-40bf-8b1a-711039e54a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-14 12:54:01.095502: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from trainer import BasicTrainer\n",
    "from model.base_par import DepParser\n",
    "from utils import arc_rel_loss, uas_las, to_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5364156f-4402-4b34-b357-90cd4683ed92",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67c8c13e-2a29-4c4a-a0c0-bc88089e4395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_file = '../data/train_50.json'\n",
    "    codt_train_file = '../codt/train/BC-Train-full.conll'\n",
    "    codt_dev_file = '../codt/dev/BC-Dev.conll'\n",
    "    # codt_train_file = '../aug/codt/codt_train_fixed.conll'\n",
    "    # codt_dev_file = '../aug/codt/codt_dev.conll'\n",
    "    test_file = '../data/test.json'\n",
    "    plm = 'hfl/chinese-electra-180g-base-discriminator'\n",
    "    shots = [50]\n",
    "    num_epochs = 15\n",
    "    batch_size = 32\n",
    "    plm_lr = 2e-5\n",
    "    head_lr = 1e-4\n",
    "    weight_decay = 0.01\n",
    "    dropout = 0.2\n",
    "    grad_clip = 2\n",
    "    scheduler = 'linear'\n",
    "    warmup_ratio = 0.1\n",
    "    num_early_stop = 3\n",
    "    max_length = 160\n",
    "    num_labels = 35\n",
    "    hidden_size = 400\n",
    "    print_every_ratio = 0.5\n",
    "    cuda = True\n",
    "    fp16 = True\n",
    "    eval_strategy = 'epoch'\n",
    "    mode = 'inference'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41572a14-d03b-45cd-a1cd-c9679537dd07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the seed closest to the average result\n",
    "shot2seed = {5:40, 10:44, 20:41, 40:44, 50:42}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6277ca01-a7d3-46a3-80af-414b8f17a764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Logger logger (INFO)>\n",
      "<class 'logging.Logger'>\n",
      "2022-12-14T12:54:02.840797\n"
     ]
    }
   ],
   "source": [
    "logger = logging.getLogger(\"logger\")\n",
    "logger.setLevel(logging.INFO)\n",
    "print(logger)\n",
    "print(type(logger))\n",
    "\n",
    "if CFG.mode == 'training':\n",
    "    fh = logging.FileHandler(filename=f\"../results/few_shot/with_codt_sampled/training.log\",mode='w')\n",
    "else:\n",
    "    fh = logging.FileHandler(filename=f\"../results/few_shot/with_codt_sampled/inference_inter.log\",mode='w')\n",
    "logger.addHandler(fh)\n",
    "\n",
    "time_now = datetime.datetime.now().isoformat()\n",
    "print(time_now)\n",
    "logger.info(f'=-=-=-=-=-=-=-=-={time_now}=-=-=-=-=-=-=-=-=-=')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76bcaccb-18d3-4c60-a99f-c9ba2ac5687b",
   "metadata": {},
   "source": [
    "## Seed and Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fc6cbe6-687d-423a-a727-b81ea81fed21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    np.random.seed(seed%(2**32-1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9ba0a24-f554-4f67-9459-0938a67c58c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e75bf13-f8bf-4f44-9fd3-022f474f1dae",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72b2700c-7f24-4014-ba65-2f4fbd5362f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_dct = {\n",
    "    'root': '根节点',\n",
    "    'sasubj-obj': '同主同宾',\n",
    "    'sasubj': '同主语',\n",
    "    'dfsubj': '不同主语',\n",
    "    'subj': '主语',\n",
    "    'subj-in': '内部主语',\n",
    "    'obj': '宾语',\n",
    "    'pred': '谓语',\n",
    "    'att': '定语',\n",
    "    'adv': '状语',\n",
    "    'cmp': '补语',\n",
    "    'coo': '并列',\n",
    "    'pobj': '介宾',\n",
    "    'iobj': '间宾',\n",
    "    'de': '的',\n",
    "    'adjct': '附加',\n",
    "    'app': '称呼',\n",
    "    'exp': '解释',\n",
    "    'punc': '标点',\n",
    "    'frag': '片段',\n",
    "    'repet': '重复',\n",
    "    # rst\n",
    "    'attr': '归属',\n",
    "    'bckg': '背景',\n",
    "    'cause': '因果',\n",
    "    'comp': '比较',\n",
    "    'cond': '状况',\n",
    "    'cont': '对比',\n",
    "    'elbr': '阐述',\n",
    "    'enbm': '目的',\n",
    "    'eval': '评价',\n",
    "    'expl': '解释-例证',\n",
    "    'joint': '联合',\n",
    "    'manner': '方式',\n",
    "    'rstm': '重申',\n",
    "    'temp': '时序',\n",
    "    # 'tp-chg': '主题变更',\n",
    "    # 'prob-sol': '问题-解决',\n",
    "    # 'qst-ans': '疑问-回答',\n",
    "    # 'stm-rsp': '陈述-回应',\n",
    "    # 'req-proc': '需求-处理',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35cf9c30-727a-489d-97e6-efc33c2bf5a3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'root': 0, 'sasubj-obj': 1, 'sasubj': 2, 'dfsubj': 3, 'subj': 4, 'subj-in': 5, 'obj': 6, 'pred': 7, 'att': 8, 'adv': 9, 'cmp': 10, 'coo': 11, 'pobj': 12, 'iobj': 13, 'de': 14, 'adjct': 15, 'app': 16, 'exp': 17, 'punc': 18, 'frag': 19, 'repet': 20, 'attr': 21, 'bckg': 22, 'cause': 23, 'comp': 24, 'cond': 25, 'cont': 26, 'elbr': 27, 'enbm': 28, 'eval': 29, 'expl': 30, 'joint': 31, 'manner': 32, 'rstm': 33, 'temp': 34}\n"
     ]
    }
   ],
   "source": [
    "rel2id = {key:idx for idx, key in enumerate(rel_dct.keys())}\n",
    "print(rel2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f0d8e193-ac84-4024-9890-3564fe797318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.plm)\n",
    "print(len(tokenizer))\n",
    "\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f0dfad-12d1-4546-b05e-ad36bffaa720",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependency():\n",
    "    def __init__(self, idx, word, head, rel):\n",
    "        self.id = idx\n",
    "        self.word = word\n",
    "        self.head = int(head)\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self):\n",
    "        # example:  1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        values = [str(self.idx), self.word, \"_\", \"_\", \"_\", \"_\", str(self.head), self.rel, \"_\", \"_\"]\n",
    "        return '\\t'.join(values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.word}, {self.head}, {self.rel})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3154859d-3591-4177-94ef-137822dc4caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annoted(data_file, data_ids):\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    sample_lst:List[List[Dependency]] = []\n",
    "    \n",
    "    for i, d in enumerate(data):\n",
    "        if i not in data_ids:\n",
    "            continue\n",
    "        rel_dct = {}\n",
    "        for tripple in d['relationship']:\n",
    "            head, rel, tail = tripple\n",
    "            head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "            tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "            if head_uttr_idx != tail_uttr_idx:\n",
    "                continue\n",
    "            \n",
    "            if not rel_dct.get(head_uttr_idx, None):\n",
    "                rel_dct[head_uttr_idx] = {tail_word_idx: [head_word_idx, rel]}\n",
    "            else:\n",
    "                rel_dct[head_uttr_idx][tail_word_idx] = [head_word_idx, rel]\n",
    "            \n",
    "        for item in d['dialog']:\n",
    "            turn = item['turn']\n",
    "            utterance = item['utterance']\n",
    "            # dep_lst:List[Dependency] = [Dependency(0, '[root]', -1, '_')]\n",
    "            dep_lst:List[Dependency] = []\n",
    "            \n",
    "            for word_idx, word in enumerate(utterance.split(' ')):\n",
    "                head_word_idx, rel = rel_dct[turn].get(word_idx + 1, [word_idx, 'adjct'])  # some word annoted missed, padded with last word and 'adjct'\n",
    "                dep_lst.append(Dependency(word_idx + 1, word, head_word_idx, rel))  # start from 1\n",
    "            \n",
    "            sample_lst.append(dep_lst)\n",
    "        \n",
    "    return sample_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d002333-0c47-4c22-b70f-1a5cdd9eae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_codt(data_file: str):\n",
    "    # id, form, tag, head, rel\n",
    "#     sentence:List[Dependency] = [Dependency('0', '<root>', '_', '0', '_')]\n",
    "    sentence:List[Dependency] = []\n",
    "\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        # data example: 1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        for line in f.readlines():\n",
    "            toks = line.split()\n",
    "            if len(toks) == 0 and len(sentence) != 0:\n",
    "                yield sentence\n",
    "#                 sentence = [Dependency('0', '<root>', '_', '0', '_')]\n",
    "                sentence = []\n",
    "            elif len(toks) == 10:\n",
    "                if toks[8] != '_':\n",
    "                    dep = Dependency(toks[0], toks[1], toks[8], toks[9])\n",
    "                else:\n",
    "                    dep = Dependency(toks[0], toks[1], toks[6], toks[7])\n",
    "                sentence.append(dep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "13ac09de-1f8b-4614-aab5-ba4f2bc9596a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DialogDataset(Dataset):\n",
    "    def __init__(self, cfg, data_file, data_ids, train, with_codt_sampled=True):\n",
    "        self.cfg = cfg\n",
    "        self.data_file = data_file\n",
    "        self.train = train\n",
    "        self.with_codt_sampled = with_codt_sampled\n",
    "        \n",
    "        self.inputs, self.offsets, self.heads, self.rels, self.masks = self.read_data(data_ids)\n",
    "        \n",
    "    def read_data(self, data_ids):\n",
    "        inputs, offsets = [], []\n",
    "        tags, heads, rels, masks = [], [], [], []\n",
    "        \n",
    "        codt_file = self.cfg.codt_train_file if self.train else self.cfg.codt_dev_file\n",
    "        # diag_train = '../aug/diag_weakcodt_sampled_new/diag_train_merged.conll'\n",
    "        # diag_dev = '../aug/diag_weakcodt_sampled_new/diag_dev_sampled_new.conll'\n",
    "        diag_train = '../aug/diag_codt_sampled_new/diag_train_sampled.conll'\n",
    "        diag_dev = '../aug/diag_codt_sampled_new/diag_dev_sampled.conll'\n",
    "        diag_file = diag_train if self.train else diag_dev\n",
    "        \n",
    "        c_lst = []\n",
    "        for i in range(2):   \n",
    "            c_lst.append(load_annoted(self.data_file, data_ids))\n",
    "        c = chain(*c_lst)\n",
    "        if self.with_codt_sampled:\n",
    "            # data_iter = chain(c, load_codt(codt_file), c, load_codt(diag_file), c)\n",
    "            data_iter = chain(c, load_codt(codt_file))\n",
    "        else:\n",
    "            data_iter = load_annoted(self.data_file, data_ids)\n",
    "        for deps in tqdm(data_iter):\n",
    "            # another sentence\n",
    "            seq_len = len(deps)\n",
    "\n",
    "            word_lst = [] \n",
    "            head_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            rel_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            mask_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            for i, dep in enumerate(deps):\n",
    "                if i == seq_len or i + 1== self.cfg.max_length:\n",
    "                    break\n",
    "\n",
    "                word_lst.append(dep.word)\n",
    "\n",
    "                if dep.head != -1 and dep.rel in rel2id.keys() and dep.head + 1 < self.cfg.max_length:\n",
    "                    head_tokens[i+1] = dep.head\n",
    "                    mask_tokens[i+1] = 1\n",
    "                    rel_tokens[i+1] = rel2id[dep.rel]\n",
    "\n",
    "            tokenized = tokenizer.encode_plus(word_lst, \n",
    "                                              padding='max_length', \n",
    "                                              truncation=True,\n",
    "                                              max_length=self.cfg.max_length, \n",
    "                                              return_offsets_mapping=True, \n",
    "                                              return_tensors='pt',\n",
    "                                              is_split_into_words=True)\n",
    "            inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "                           \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                           \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                          })\n",
    "\n",
    "            sentence_word_idx = []\n",
    "            for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "                if start == 0 and end != 0:\n",
    "                    sentence_word_idx.append(idx)\n",
    "\n",
    "            if len(sentence_word_idx) < self.cfg.max_length - 1:\n",
    "                sentence_word_idx.extend([0]* (self.cfg.max_length - 1 - len(sentence_word_idx)))\n",
    "            offsets.append(torch.as_tensor(sentence_word_idx))\n",
    "\n",
    "            heads.append(head_tokens)\n",
    "            rels.append(rel_tokens)\n",
    "            masks.append(mask_tokens)\n",
    "                    \n",
    "        return inputs, offsets, heads, rels, masks\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.offsets[idx], self.heads[idx], self.rels[idx], self.masks[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e577d759-066c-4a2f-ba7e-dd4f4fea38b0",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ea7c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_codt_signal(data_file: str, return_two=False):\n",
    "    sentence:List[Dependency] = []\n",
    "\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        # data example: 1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        for line in f.readlines():\n",
    "            toks = line.split()\n",
    "            if len(toks) == 0 and len(sentence) != 0:\n",
    "                yield sentence\n",
    "                sentence = []\n",
    "            elif len(toks) == 10:                \n",
    "                if return_two:\n",
    "                    sentence.append([int(toks[2]), int(toks[3])])\n",
    "                else:\n",
    "                    sentence.append(int(toks[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b1e9a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postproc(arc_logits, rel_logits, masks_whole):\n",
    "    from constant import rel2id, punct_lst, weak_signals, weak_labels\n",
    "    \n",
    "    rel_preds = rel_logits.argmax(-1)\n",
    "    head_preds = arc_logits.argmax(-1)\n",
    "\n",
    "    origin4change = [rel2id[x] for x in ['root', 'dfsubj', 'sasubj']]\n",
    "    # origin4change.extend([i for i in range(21, 35)])\n",
    "\n",
    "    max_len = CFG.max_length\n",
    "\n",
    "    signals_new_whole = torch.Tensor()\n",
    "    heads_new_whole, rels_new_whole = torch.Tensor(), torch.Tensor()\n",
    "    for sample_idx, (deps, pred_signals) in tqdm(enumerate(zip(load_annoted(CFG.test_file, data_ids=list(range(800))), load_codt_signal('../mlm_based/diag_test.conll')))):\n",
    "        seq_len = len(deps)\n",
    "        if seq_len == 0:\n",
    "            continue\n",
    "\n",
    "        signals = torch.full(size=(max_len,), fill_value=rel2id['elbr']).int()\n",
    "        heads, rels = torch.full(size=(max_len,), fill_value=-2).int(), torch.zeros(max_len).int()\n",
    "        split, splits, signal, word_lst  = 1, [1], rel2id['elbr'], ['root']\n",
    "        for i, dep in enumerate(deps[:-1]):\n",
    "            if i + 2 >= max_len:\n",
    "                break\n",
    "\n",
    "            word = dep.word\n",
    "            word_lst.append(word)\n",
    "\n",
    "            try:\n",
    "                signal = pred_signals[i]\n",
    "            except IndexError:\n",
    "                signal = pred_signals[len(pred_signals) - 1]\n",
    "\n",
    "            if word in punct_lst and deps[i+1].word not in punct_lst:\n",
    "                if i + 2 - split > 2:  # set 2 to the min length of edu\n",
    "                    signals[split:i+2] = signal\n",
    "                    # signal = None\n",
    "                split = i + 2\n",
    "                splits.append(split)\n",
    "\n",
    "        splits.append(len(deps))\n",
    "\n",
    "        # add the last data\n",
    "        if i + 1 < max_len:\n",
    "            signal = pred_signals[-1]\n",
    "            word_lst.append(word)\n",
    "\n",
    "        heads = head_preds[sample_idx]\n",
    "        heads.masked_fill_(mask=~masks_whole[sample_idx].bool(), value=-2)\n",
    "\n",
    "        rels = rel_preds[sample_idx]\n",
    "        rels.masked_fill_(mask=~masks_whole[sample_idx].bool(), value=-2)\n",
    "\n",
    "        cnt, attr, = -1, False\n",
    "        for idx, head in enumerate(heads[1:]):\n",
    "            if head == -2:\n",
    "                break\n",
    "            if head == -1:\n",
    "                continue\n",
    "\n",
    "            if len(splits) > 2 and idx + 1 >= splits[cnt+1]:\n",
    "                cnt += 1\n",
    "\n",
    "            if ((len(splits) > 2 and (head < splits[cnt] or head >= splits[cnt+1])) or idx - head > 0) and rels[idx + 1] in origin4change:  # cross 'edu'\n",
    "\n",
    "                rels[idx+1] = signals[idx+1]\n",
    "\n",
    "                if rels[idx + 1] in [rel2id['cond']]:  # reverse\n",
    "                    tmp_heads = heads.clone()\n",
    "                    tmp_heads[:splits[cnt+1]] = 0\n",
    "                    head_idx = [idx + 1]\n",
    "                    tail_idx = (tmp_heads == idx + 1).nonzero()  # find tail index\n",
    "                    if len(tail_idx) == 0:  # ring or fail\n",
    "                        # unchange\n",
    "                        tail_idx = [idx + 1]\n",
    "                        head_idx = (heads == idx + 1).nonzero() if head_idx == tail_idx else head_idx\n",
    "                    elif len(head_idx) != 0:\n",
    "                        heads[tail_idx[0]] = 0\n",
    "                        heads[head_idx[0]] = tail_idx[0]\n",
    "\n",
    "                # special cases\n",
    "                if word_lst[idx+1] == '好' and word_lst[idx] in ['你', '您']:  # reverse\n",
    "                    tmp_heads = heads.clone()\n",
    "                    tmp_heads[:splits[cnt+1]] = 0\n",
    "                    tail_idx = (tmp_heads == idx + 1).nonzero()  # find tail index\n",
    "                    if len(tail_idx) != 0:  \n",
    "                        heads[tail_idx[0]] = 0\n",
    "                        heads[idx + 1] = tail_idx[0]\n",
    "                        rels[idx + 1] = rel2id['elbr']\n",
    "\n",
    "            if not attr and rels[idx + 1] in [rel2id['obj']] and signals[idx+1] == rel2id['attr']:\n",
    "                rels[idx+1] = signals[idx+1]\n",
    "                attr = True\n",
    "\n",
    "        rels.masked_fill_(heads == 0, 0)  # root\n",
    "        heads[0] = 0\n",
    "        heads[1:].masked_fill_(heads[1:] == -2, 0)\n",
    "\n",
    "        heads_new_whole = torch.cat([heads_new_whole, heads.unsqueeze(0)])\n",
    "        rels_new_whole = torch.cat([rels_new_whole, rels.unsqueeze(0)])\n",
    "        signals_new_whole = torch.cat([signals_new_whole, signals.unsqueeze(0)])\n",
    "\n",
    "    return heads_new_whole, rels_new_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "009eb28a-cbf0-473c-89ec-ea254ec7b5fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_fn(model, eval_iter):\n",
    "    arc_logits, rel_logits = torch.Tensor(), torch.Tensor()\n",
    "    heads_whole, rels_whole, masks_whole = torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "    for batch in eval_iter:\n",
    "        inputs, offsets, heads, rels, masks = batch\n",
    "\n",
    "        inputs_cuda = {}\n",
    "        for key, value in inputs.items():\n",
    "            inputs_cuda[key] = value.cuda()\n",
    "        inputs = inputs_cuda\n",
    "\n",
    "        offsets, heads, rels, masks = to_cuda(data=(offsets, heads, rels, masks))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            arc_logit, rel_logit = model(inputs, offsets, heads, rels, masks, evaluate=True)\n",
    "\n",
    "        arc_logit[:, torch.arange(arc_logit.size()[1]), torch.arange(arc_logit.size()[2])] = -1e4\n",
    "\n",
    "        arc_logits = torch.cat([arc_logits, arc_logit.cpu()])\n",
    "        rel_logits = torch.cat([rel_logits, rel_logit.cpu()])\n",
    "\n",
    "        heads_whole = torch.cat([heads_whole, heads.cpu()])\n",
    "        rels_whole = torch.cat([rels_whole, rels.cpu()])\n",
    "        masks_whole = torch.cat([masks_whole, masks.cpu()])\n",
    "\n",
    "    # rel_preds = rel_logits.argmax(-1)\n",
    "    # head_pred = arc_logits.argmax(-1)\n",
    "    head_preds, rel_preds = postproc(arc_logits, rel_logits, masks_whole)\n",
    "\n",
    "    return arc_logits, rel_logits, head_preds, rel_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b3228059-97d5-46dc-bf91-cdfe3e21094f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20086/20086 [00:36<00:00, 551.07it/s]\n"
     ]
    }
   ],
   "source": [
    "test_dataset = DialogDataset(CFG, CFG.test_file, list(range(800)), train=False, with_codt_sampled=False)\n",
    "test_iter = DataLoader(test_dataset, batch_size=CFG.batch_size * 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86c99524",
   "metadata": {},
   "source": [
    "# Inter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "21eef797",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_inter(data_file):\n",
    "    with open(data_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "        \n",
    "    signal_iter = load_codt_signal('../mlm_based/diag_test.conll', return_two=True)\n",
    "\n",
    "    sample_lst:List[List[Dependency]] = []\n",
    "    # for d, pred_signals in tqdm(zip(data, load_codt_signal('../prompt_based/diag_test.conll', idx=3))):\n",
    "    for d in data:\n",
    "        rel_dct = {}\n",
    "        for tripple in d['relationship']:\n",
    "            head, rel, tail = tripple\n",
    "            head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "            tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "            \n",
    "            if rel == 'root' and head_uttr_idx != 0: # ignore root\n",
    "                continue\n",
    "                 \n",
    "            if not rel_dct.get(tail_uttr_idx, None):\n",
    "                rel_dct[tail_uttr_idx] = {tail_word_idx: [head, rel]}\n",
    "            else:\n",
    "                rel_dct[tail_uttr_idx][tail_word_idx] = [head, rel]\n",
    "                \n",
    "        sent_lens_accum = [1]\n",
    "        for i, item in enumerate(d['dialog']):\n",
    "            utterance = item['utterance']\n",
    "            sent_lens_accum.append(sent_lens_accum[i] + len(utterance.split(' ')) + 1)\n",
    "        sent_lens_accum[0] = 0\n",
    "        \n",
    "        dep_lst:List[Dependency] = []\n",
    "        role_lst:List[str] = []\n",
    "        weak_signal = []\n",
    "        for item in d['dialog']:\n",
    "            turn = item['turn']\n",
    "            utterance = item['utterance']\n",
    "\n",
    "            pred_signals = next(signal_iter)\n",
    "\n",
    "            role = '[ans]' if item['speaker'] == 'A' else '[qst]'\n",
    "            dep_lst.append(Dependency(sent_lens_accum[turn], role, -1, '_'))  \n",
    "            \n",
    "            tmp_signal = []\n",
    "            for word_idx, word in enumerate(utterance.split(' ')):\n",
    "                tail2head = rel_dct.get(turn, {1: [f'{turn}-{word_idx}', 'adjct']})\n",
    "                head, rel = tail2head.get(word_idx + 1, [f'{turn}-{word_idx}', 'adjct'])  # some word annoted missed, padded with last word and 'adjct'\n",
    "                head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "                \n",
    "                # only parse cross-utterance\n",
    "                if turn != head_uttr_idx:\n",
    "                    dep_lst.append(Dependency(sent_lens_accum[turn] + word_idx + 1, word, sent_lens_accum[head_uttr_idx] + head_word_idx, rel))  # add with accumulated length\n",
    "                else:\n",
    "                    dep_lst.append(Dependency(sent_lens_accum[turn] + word_idx + 1, word, -1, '_')) \n",
    "\n",
    "                try:\n",
    "                    signal1, signal2 = pred_signals[i]\n",
    "                except IndexError:\n",
    "                    signal1, signal2 = pred_signals[len(pred_signals) - 1]\n",
    "                \n",
    "                tmp_signal = [signal1, signal2]\n",
    "                \n",
    "                # if word in weak_signal_dct.keys():\n",
    "                #     tmp_signal.append(weak_signal_dct[word])\n",
    "\n",
    "            if len(tmp_signal) != 0:\n",
    "                # weak_signal.append(tmp_signal[-1])  # choose the last\n",
    "                weak_signal.append(tmp_signal)  # choose the last\n",
    "            else:\n",
    "                weak_signal.append(-1)\n",
    "            role_lst.append(item['speaker'])        \n",
    "        sample_lst.append([dep_lst, role_lst, weak_signal])\n",
    "        \n",
    "    return sample_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b4d50de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterDataset(Dataset):\n",
    "    def __init__(self, cfg):\n",
    "        self.cfg = cfg\n",
    "        self.inputs, self.offsets, self.heads, self.rels, self.masks, self.speakers, self.signs = self.read_data()\n",
    "        \n",
    "    def read_data(self):\n",
    "        inputs, offsets = [], []\n",
    "        tags, heads, rels, masks, speakers, signs = [], [], [], [], [], []\n",
    "                \n",
    "        for idx, (deps, roles, sign) in enumerate(load_inter(self.cfg.data_file)):\n",
    "            seq_len = len(deps)\n",
    "            signs.append(sign)\n",
    "\n",
    "            word_lst = [] \n",
    "            head_tokens = np.zeros(1024, dtype=np.int64)  # same as root index is 0, constrainting by mask \n",
    "            rel_tokens = np.zeros(1024, dtype=np.int64)\n",
    "            mask_tokens = np.zeros(1024, dtype=np.int64)\n",
    "            for i, dep in enumerate(deps):\n",
    "                if i == seq_len or i + 1== 1024:\n",
    "                    break\n",
    "\n",
    "                word_lst.append(dep.word)\n",
    "                \n",
    "                if int(dep.head) == -1 or int(dep.head) + 1 >= 1024:\n",
    "                    head_tokens[i+1] = 0\n",
    "                    mask_tokens[i+1] = 0\n",
    "                else:\n",
    "                    head_tokens[i+1] = int(dep.head)\n",
    "                    mask_tokens[i+1] = 1\n",
    "#                     head_tokens[i] = dep.head if dep.head != '_' else 0\n",
    "                rel_tokens[i+1] = rel2id.get(dep.rel, 0)\n",
    "\n",
    "            tokenized = tokenizer.encode_plus(word_lst, \n",
    "                                              padding='max_length', \n",
    "                                              truncation=True,\n",
    "                                              max_length=1024, \n",
    "                                              return_offsets_mapping=True, \n",
    "                                              return_tensors='pt',\n",
    "                                              is_split_into_words=True)\n",
    "            inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "                          \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                           \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                          })\n",
    "\n",
    "            sentence_word_idx = []\n",
    "            for idx, (start, end) in enumerate(tokenized.offset_mapping[0][1:]):\n",
    "                if start == 0 and end != 0:\n",
    "                    sentence_word_idx.append(idx)\n",
    "\n",
    "            if len(sentence_word_idx) < 1024 - 1:\n",
    "                sentence_word_idx.extend([0]* (1024 - 1 - len(sentence_word_idx)))\n",
    "            offsets.append(torch.as_tensor(sentence_word_idx))\n",
    "\n",
    "            heads.append(head_tokens)\n",
    "            rels.append(rel_tokens)\n",
    "            masks.append(mask_tokens)\n",
    "            speakers.append(roles)\n",
    "                    \n",
    "        return inputs, offsets, heads, rels, masks, speakers, signs\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.offsets[idx], self.heads[idx], self.rels[idx], self.masks[idx], self.speakers[idx], self.signs[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d329da66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_root(rel_preds, masks_whole):\n",
    "    root_ids = []\n",
    "    # for rel_pred, mask in zip(rel_preds, masks_whole):\n",
    "    for rel_pred, mask in zip(rel_preds, masks_whole):\n",
    "        try:\n",
    "            root_idx = (((rel_pred == 0) * mask) != 0).nonzero()[0].item()\n",
    "        except IndexError: # no root\n",
    "            root_idx = 2\n",
    "        root_ids.append(root_idx)\n",
    "\n",
    "    return root_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dff36e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_inter(inter_dataloader, masks_whole, root_ids):\n",
    "    cnt = 0\n",
    "\n",
    "    inter_heads_whole, inter_rels_whole, inter_masks_whole = torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "    inter_heads_preds, inter_rels_preds = torch.Tensor(), torch.Tensor()\n",
    "    for batch in inter_dataloader:\n",
    "        inputs, offsets, heads, rels, masks, speakers, signs = batch\n",
    "        inter_head_preds = torch.zeros_like(heads, dtype=int)\n",
    "        inter_rel_preds = torch.zeros_like(rels, dtype=int)\n",
    "\n",
    "        inter_heads_whole = torch.cat([inter_heads_whole, heads])\n",
    "        inter_rels_whole = torch.cat([inter_rels_whole, rels])\n",
    "        inter_masks_whole = torch.cat([inter_masks_whole, masks])\n",
    "\n",
    "        accum = 1\n",
    "        for i, speakr in enumerate(speakers[1:]):\n",
    "            seq_len = masks_whole[cnt].sum().item() + 1\n",
    "\n",
    "            if speakr == speakers[i]:\n",
    "                rel = signs[i][0]\n",
    "            else:\n",
    "                rel = signs[i][1]\n",
    "            \n",
    "            head_idx = int(root_ids[cnt] + accum) if i > 0 else root_ids[cnt]\n",
    "            tail_idx = int(root_ids[cnt+1] + accum + seq_len)\n",
    "            \n",
    "            inter_head_preds[0][tail_idx] = head_idx\n",
    "            inter_rel_preds[0][tail_idx] = rel\n",
    "\n",
    "            cnt += 1\n",
    "            accum += seq_len\n",
    "\n",
    "        cnt += 1\n",
    "\n",
    "        inter_heads_preds = torch.cat([inter_heads_preds, inter_head_preds])\n",
    "        inter_rels_preds = torch.cat([inter_rels_preds, inter_rel_preds])\n",
    "\n",
    "    arc_logits_correct = (inter_heads_preds == inter_heads_whole).long() * inter_masks_whole\n",
    "    rel_logits_correct = (inter_rels_preds == inter_rels_whole).long() * arc_logits_correct\n",
    "\n",
    "    logger.info(rel_logits_correct.sum() / inter_masks_whole.long().sum())\n",
    "    logger.info(arc_logits_correct.sum() / inter_masks_whole.long().sum())\n",
    "\n",
    "    return arc_logits_correct, rel_logits_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b55a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_dataset = InterDataset(CFG)\n",
    "inter_dataloader = DataLoader(inter_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7294595",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shot in CFG.shots:\n",
    "    logger.info(f'----------------Shot: {shot}----------------')\n",
    "    seed = shot2seed[shot]\n",
    "    seed_everything(seed)\n",
    "    \n",
    "    model = DepParser(CFG)\n",
    "    model.load_state_dict(torch.load(f'../results/few_shot/with_codt_sampled/model_shot_{shot}.bin'))\n",
    "    model = model.cuda()\n",
    "        \n",
    "    arc_logits, rel_logits = torch.Tensor(), torch.Tensor()\n",
    "    heads_whole, rels_whole, masks_whole = torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "    for batch in test_iter:\n",
    "        inputs, offsets, heads, rels, masks = batch\n",
    "\n",
    "        inputs_cuda = {}\n",
    "        for key, value in inputs.items():\n",
    "            inputs_cuda[key] = value.cuda()\n",
    "        inputs = inputs_cuda\n",
    "\n",
    "        offsets, heads, rels, masks = to_cuda(data=(offsets, heads, rels, masks))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            arc_logit, rel_logit = model(inputs, offsets, heads, rels, masks, evaluate=True)\n",
    "\n",
    "        arc_logit[:, torch.arange(arc_logit.size()[1]), torch.arange(arc_logit.size()[2])] = -1e4\n",
    "\n",
    "        arc_logits = torch.cat([arc_logits, arc_logit.cpu()])\n",
    "        rel_logits = torch.cat([rel_logits, rel_logit.cpu()])\n",
    "\n",
    "        heads_whole = torch.cat([heads_whole, heads.cpu()])\n",
    "        rels_whole = torch.cat([rels_whole, rels.cpu()])\n",
    "        masks_whole = torch.cat([masks_whole, masks.cpu()])\n",
    "\n",
    "    # rel_preds = rel_logits.argmax(-1)\n",
    "    # head_pred = arc_logits.argmax(-1)\n",
    "    head_preds, rel_preds = postproc(arc_logits, rel_logits, masks_whole)\n",
    "\n",
    "    root_ids = get_root(rel_preds, masks_whole)\n",
    "\n",
    "    arc_correct_inter, rel_correct_inter = eval_inter(inter_dataloader, masks_whole, root_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f1da6d51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 16],\n",
      "        [ 22],\n",
      "        [ 33],\n",
      "        [ 38],\n",
      "        [ 47],\n",
      "        [ 51],\n",
      "        [ 53],\n",
      "        [ 59],\n",
      "        [ 64],\n",
      "        [ 69],\n",
      "        [ 73],\n",
      "        [100],\n",
      "        [105],\n",
      "        [117],\n",
      "        [134],\n",
      "        [146],\n",
      "        [161]])\n",
      "tensor([[ 20],\n",
      "        [ 28],\n",
      "        [ 37],\n",
      "        [ 55],\n",
      "        [ 64],\n",
      "        [ 75],\n",
      "        [ 89],\n",
      "        [ 97],\n",
      "        [103],\n",
      "        [116],\n",
      "        [133],\n",
      "        [139],\n",
      "        [147],\n",
      "        [155],\n",
      "        [172],\n",
      "        [177],\n",
      "        [186]])\n",
      "tensor([[ 14],\n",
      "        [ 25],\n",
      "        [ 38],\n",
      "        [ 42],\n",
      "        [ 48],\n",
      "        [ 54],\n",
      "        [ 63],\n",
      "        [ 69],\n",
      "        [ 74],\n",
      "        [ 88],\n",
      "        [104],\n",
      "        [108],\n",
      "        [116]])\n",
      "tensor([[  7],\n",
      "        [ 13],\n",
      "        [ 20],\n",
      "        [ 29],\n",
      "        [ 33],\n",
      "        [ 82],\n",
      "        [ 89],\n",
      "        [ 97],\n",
      "        [102],\n",
      "        [116],\n",
      "        [121],\n",
      "        [127]])\n",
      "tensor([[  9],\n",
      "        [ 15],\n",
      "        [ 20],\n",
      "        [ 23],\n",
      "        [ 30],\n",
      "        [ 38],\n",
      "        [ 65],\n",
      "        [ 68],\n",
      "        [ 84],\n",
      "        [ 96],\n",
      "        [111],\n",
      "        [129],\n",
      "        [132],\n",
      "        [144],\n",
      "        [156],\n",
      "        [167],\n",
      "        [171],\n",
      "        [180],\n",
      "        [190],\n",
      "        [202],\n",
      "        [209],\n",
      "        [214],\n",
      "        [221],\n",
      "        [228],\n",
      "        [232],\n",
      "        [238],\n",
      "        [246],\n",
      "        [250],\n",
      "        [253],\n",
      "        [258],\n",
      "        [265],\n",
      "        [267],\n",
      "        [270]])\n",
      "tensor([[  8],\n",
      "        [ 29],\n",
      "        [ 50],\n",
      "        [ 56],\n",
      "        [ 69],\n",
      "        [ 75],\n",
      "        [ 79],\n",
      "        [ 83],\n",
      "        [ 95],\n",
      "        [126],\n",
      "        [138],\n",
      "        [144],\n",
      "        [149],\n",
      "        [158],\n",
      "        [170],\n",
      "        [173],\n",
      "        [176],\n",
      "        [183],\n",
      "        [188],\n",
      "        [195],\n",
      "        [200],\n",
      "        [207],\n",
      "        [213],\n",
      "        [216],\n",
      "        [240],\n",
      "        [245],\n",
      "        [251],\n",
      "        [294],\n",
      "        [299],\n",
      "        [302],\n",
      "        [305],\n",
      "        [315],\n",
      "        [319]])\n",
      "tensor([[ 16],\n",
      "        [ 23],\n",
      "        [ 26],\n",
      "        [ 29],\n",
      "        [ 40],\n",
      "        [ 44],\n",
      "        [ 55],\n",
      "        [ 64],\n",
      "        [ 68],\n",
      "        [ 72],\n",
      "        [ 75],\n",
      "        [ 79],\n",
      "        [ 84],\n",
      "        [ 89],\n",
      "        [ 96],\n",
      "        [110],\n",
      "        [114],\n",
      "        [116],\n",
      "        [146],\n",
      "        [153],\n",
      "        [163],\n",
      "        [173],\n",
      "        [177],\n",
      "        [182],\n",
      "        [189],\n",
      "        [193]])\n",
      "tensor([[  4],\n",
      "        [  8],\n",
      "        [ 28],\n",
      "        [ 40],\n",
      "        [ 44],\n",
      "        [ 61],\n",
      "        [ 63],\n",
      "        [ 71],\n",
      "        [ 88],\n",
      "        [102],\n",
      "        [113],\n",
      "        [119],\n",
      "        [123],\n",
      "        [129],\n",
      "        [133],\n",
      "        [143],\n",
      "        [170],\n",
      "        [180],\n",
      "        [191],\n",
      "        [201],\n",
      "        [206],\n",
      "        [217],\n",
      "        [236],\n",
      "        [253],\n",
      "        [259],\n",
      "        [264],\n",
      "        [266]])\n",
      "tensor([[12],\n",
      "        [24],\n",
      "        [28],\n",
      "        [35],\n",
      "        [44],\n",
      "        [51],\n",
      "        [58],\n",
      "        [70],\n",
      "        [74],\n",
      "        [77],\n",
      "        [94]])\n",
      "tensor([[  9],\n",
      "        [ 15],\n",
      "        [ 24],\n",
      "        [ 38],\n",
      "        [ 54],\n",
      "        [ 69],\n",
      "        [ 76],\n",
      "        [ 81],\n",
      "        [ 98],\n",
      "        [108],\n",
      "        [120],\n",
      "        [122],\n",
      "        [124],\n",
      "        [146],\n",
      "        [194],\n",
      "        [210],\n",
      "        [215],\n",
      "        [232],\n",
      "        [240],\n",
      "        [248],\n",
      "        [260],\n",
      "        [268],\n",
      "        [274]])\n",
      "tensor([[  4],\n",
      "        [ 20],\n",
      "        [ 26],\n",
      "        [ 31],\n",
      "        [ 41],\n",
      "        [ 45],\n",
      "        [ 53],\n",
      "        [ 62],\n",
      "        [ 68],\n",
      "        [130],\n",
      "        [150],\n",
      "        [152],\n",
      "        [157],\n",
      "        [171]])\n",
      "tensor([[ 13],\n",
      "        [ 18],\n",
      "        [ 30],\n",
      "        [ 54],\n",
      "        [ 60],\n",
      "        [ 63],\n",
      "        [ 74],\n",
      "        [ 77],\n",
      "        [ 84],\n",
      "        [100],\n",
      "        [109],\n",
      "        [116],\n",
      "        [125],\n",
      "        [136],\n",
      "        [141],\n",
      "        [155],\n",
      "        [167],\n",
      "        [176],\n",
      "        [191],\n",
      "        [199]])\n",
      "tensor([[ 11],\n",
      "        [ 18],\n",
      "        [ 23],\n",
      "        [ 32],\n",
      "        [ 42],\n",
      "        [ 62],\n",
      "        [ 75],\n",
      "        [ 83],\n",
      "        [ 87],\n",
      "        [ 99],\n",
      "        [104],\n",
      "        [110],\n",
      "        [112],\n",
      "        [124],\n",
      "        [129],\n",
      "        [160],\n",
      "        [165],\n",
      "        [184],\n",
      "        [202],\n",
      "        [205],\n",
      "        [209],\n",
      "        [213],\n",
      "        [221],\n",
      "        [240],\n",
      "        [269]])\n",
      "tensor([[  5],\n",
      "        [  8],\n",
      "        [ 14],\n",
      "        [ 28],\n",
      "        [ 40],\n",
      "        [ 47],\n",
      "        [ 56],\n",
      "        [ 58],\n",
      "        [ 75],\n",
      "        [ 92],\n",
      "        [ 99],\n",
      "        [102],\n",
      "        [108],\n",
      "        [114],\n",
      "        [121],\n",
      "        [127],\n",
      "        [131],\n",
      "        [134],\n",
      "        [140],\n",
      "        [144],\n",
      "        [162],\n",
      "        [164],\n",
      "        [169],\n",
      "        [174],\n",
      "        [185],\n",
      "        [190],\n",
      "        [192]])\n",
      "tensor([[ 18],\n",
      "        [ 23],\n",
      "        [ 45],\n",
      "        [ 51],\n",
      "        [ 56],\n",
      "        [ 60],\n",
      "        [ 64],\n",
      "        [ 68],\n",
      "        [ 82],\n",
      "        [ 84],\n",
      "        [ 98],\n",
      "        [119],\n",
      "        [136],\n",
      "        [141],\n",
      "        [149],\n",
      "        [154],\n",
      "        [170],\n",
      "        [174],\n",
      "        [183],\n",
      "        [190],\n",
      "        [192],\n",
      "        [204],\n",
      "        [210],\n",
      "        [212]])\n",
      "tensor([[17],\n",
      "        [20],\n",
      "        [34],\n",
      "        [37],\n",
      "        [42],\n",
      "        [48],\n",
      "        [59],\n",
      "        [63],\n",
      "        [72],\n",
      "        [80],\n",
      "        [85]])\n",
      "tensor([[ 13],\n",
      "        [ 16],\n",
      "        [ 21],\n",
      "        [ 24],\n",
      "        [ 31],\n",
      "        [ 36],\n",
      "        [ 72],\n",
      "        [ 83],\n",
      "        [ 89],\n",
      "        [ 92],\n",
      "        [ 95],\n",
      "        [104],\n",
      "        [108],\n",
      "        [128],\n",
      "        [140],\n",
      "        [152],\n",
      "        [157],\n",
      "        [172],\n",
      "        [209],\n",
      "        [213],\n",
      "        [216],\n",
      "        [220],\n",
      "        [226],\n",
      "        [234],\n",
      "        [241],\n",
      "        [277],\n",
      "        [284],\n",
      "        [289],\n",
      "        [299],\n",
      "        [302],\n",
      "        [310],\n",
      "        [317],\n",
      "        [321],\n",
      "        [324],\n",
      "        [336],\n",
      "        [343],\n",
      "        [349],\n",
      "        [354],\n",
      "        [364],\n",
      "        [368],\n",
      "        [385],\n",
      "        [389],\n",
      "        [402],\n",
      "        [406],\n",
      "        [408],\n",
      "        [410],\n",
      "        [423],\n",
      "        [429],\n",
      "        [433],\n",
      "        [451]])\n",
      "tensor([[ 28],\n",
      "        [ 37],\n",
      "        [ 51],\n",
      "        [ 65],\n",
      "        [ 68],\n",
      "        [ 76],\n",
      "        [ 79],\n",
      "        [ 93],\n",
      "        [101],\n",
      "        [107],\n",
      "        [113],\n",
      "        [125],\n",
      "        [137]])\n",
      "tensor([[ 17],\n",
      "        [ 25],\n",
      "        [ 45],\n",
      "        [ 48],\n",
      "        [ 55],\n",
      "        [ 68],\n",
      "        [ 74],\n",
      "        [ 85],\n",
      "        [ 87],\n",
      "        [ 93],\n",
      "        [120]])\n",
      "tensor([[  8],\n",
      "        [ 12],\n",
      "        [ 17],\n",
      "        [ 29],\n",
      "        [ 41],\n",
      "        [ 44],\n",
      "        [ 51],\n",
      "        [ 53],\n",
      "        [ 70],\n",
      "        [ 76],\n",
      "        [ 83],\n",
      "        [ 95],\n",
      "        [103],\n",
      "        [126],\n",
      "        [137],\n",
      "        [149],\n",
      "        [166],\n",
      "        [180],\n",
      "        [201],\n",
      "        [218],\n",
      "        [230],\n",
      "        [237],\n",
      "        [240],\n",
      "        [243],\n",
      "        [255],\n",
      "        [262],\n",
      "        [269],\n",
      "        [278],\n",
      "        [290],\n",
      "        [302],\n",
      "        [308],\n",
      "        [323],\n",
      "        [327],\n",
      "        [335],\n",
      "        [353],\n",
      "        [358],\n",
      "        [375],\n",
      "        [381],\n",
      "        [384],\n",
      "        [419],\n",
      "        [423],\n",
      "        [466],\n",
      "        [469],\n",
      "        [475],\n",
      "        [478]])\n",
      "tensor([[ 9],\n",
      "        [16],\n",
      "        [18],\n",
      "        [25],\n",
      "        [31],\n",
      "        [34],\n",
      "        [37],\n",
      "        [43],\n",
      "        [48],\n",
      "        [54]])\n",
      "tensor([[ 14],\n",
      "        [ 17],\n",
      "        [ 20],\n",
      "        [ 22],\n",
      "        [ 31],\n",
      "        [ 42],\n",
      "        [ 45],\n",
      "        [ 47],\n",
      "        [ 50],\n",
      "        [ 66],\n",
      "        [ 85],\n",
      "        [ 87],\n",
      "        [ 90],\n",
      "        [ 93],\n",
      "        [106],\n",
      "        [111]])\n",
      "tensor([[ 18],\n",
      "        [ 27],\n",
      "        [ 36],\n",
      "        [ 43],\n",
      "        [ 68],\n",
      "        [ 77],\n",
      "        [ 80],\n",
      "        [103],\n",
      "        [111],\n",
      "        [113],\n",
      "        [120],\n",
      "        [133],\n",
      "        [135],\n",
      "        [137],\n",
      "        [145],\n",
      "        [149],\n",
      "        [151],\n",
      "        [157],\n",
      "        [163],\n",
      "        [166],\n",
      "        [174],\n",
      "        [177],\n",
      "        [181],\n",
      "        [191],\n",
      "        [196],\n",
      "        [200],\n",
      "        [204],\n",
      "        [208],\n",
      "        [210],\n",
      "        [216],\n",
      "        [228]])\n",
      "tensor([[  7],\n",
      "        [ 10],\n",
      "        [ 44],\n",
      "        [ 56],\n",
      "        [ 58],\n",
      "        [ 67],\n",
      "        [ 72],\n",
      "        [ 82],\n",
      "        [ 88],\n",
      "        [ 92],\n",
      "        [ 98],\n",
      "        [102]])\n",
      "tensor([[  7],\n",
      "        [ 12],\n",
      "        [ 22],\n",
      "        [ 32],\n",
      "        [ 75],\n",
      "        [ 91],\n",
      "        [ 94],\n",
      "        [104],\n",
      "        [111],\n",
      "        [118],\n",
      "        [126],\n",
      "        [136],\n",
      "        [142],\n",
      "        [145],\n",
      "        [149],\n",
      "        [152],\n",
      "        [167],\n",
      "        [182],\n",
      "        [191],\n",
      "        [200],\n",
      "        [213],\n",
      "        [227],\n",
      "        [236],\n",
      "        [249],\n",
      "        [252],\n",
      "        [256],\n",
      "        [274],\n",
      "        [297],\n",
      "        [304],\n",
      "        [308],\n",
      "        [320],\n",
      "        [335],\n",
      "        [353],\n",
      "        [357],\n",
      "        [369],\n",
      "        [377],\n",
      "        [399],\n",
      "        [403],\n",
      "        [413],\n",
      "        [432]])\n",
      "tensor([[10],\n",
      "        [17],\n",
      "        [19],\n",
      "        [26],\n",
      "        [39],\n",
      "        [42],\n",
      "        [52],\n",
      "        [57],\n",
      "        [73],\n",
      "        [81],\n",
      "        [90]])\n",
      "tensor([[  5],\n",
      "        [  8],\n",
      "        [ 12],\n",
      "        [ 25],\n",
      "        [ 31],\n",
      "        [ 41],\n",
      "        [ 64],\n",
      "        [ 68],\n",
      "        [ 73],\n",
      "        [ 87],\n",
      "        [ 90],\n",
      "        [ 96],\n",
      "        [115],\n",
      "        [119],\n",
      "        [133],\n",
      "        [144],\n",
      "        [183],\n",
      "        [211],\n",
      "        [225],\n",
      "        [231],\n",
      "        [242],\n",
      "        [257],\n",
      "        [260],\n",
      "        [276],\n",
      "        [281],\n",
      "        [285],\n",
      "        [290],\n",
      "        [295],\n",
      "        [299],\n",
      "        [309],\n",
      "        [312],\n",
      "        [314]])\n",
      "tensor([[ 19],\n",
      "        [ 28],\n",
      "        [ 33],\n",
      "        [ 36],\n",
      "        [ 48],\n",
      "        [ 53],\n",
      "        [ 66],\n",
      "        [111],\n",
      "        [118],\n",
      "        [126],\n",
      "        [132],\n",
      "        [139],\n",
      "        [143],\n",
      "        [148],\n",
      "        [152],\n",
      "        [159],\n",
      "        [165],\n",
      "        [169],\n",
      "        [174],\n",
      "        [188],\n",
      "        [200],\n",
      "        [214]])\n",
      "tensor([[ 12],\n",
      "        [ 17],\n",
      "        [ 29],\n",
      "        [ 34],\n",
      "        [ 41],\n",
      "        [ 45],\n",
      "        [ 57],\n",
      "        [ 64],\n",
      "        [ 76],\n",
      "        [ 85],\n",
      "        [ 88],\n",
      "        [ 92],\n",
      "        [ 95],\n",
      "        [102],\n",
      "        [109],\n",
      "        [123],\n",
      "        [159]])\n",
      "tensor([[ 10],\n",
      "        [ 19],\n",
      "        [ 22],\n",
      "        [ 42],\n",
      "        [ 45],\n",
      "        [ 65],\n",
      "        [ 82],\n",
      "        [ 85],\n",
      "        [111],\n",
      "        [114],\n",
      "        [118],\n",
      "        [126],\n",
      "        [133],\n",
      "        [138],\n",
      "        [141],\n",
      "        [145],\n",
      "        [150],\n",
      "        [154]])\n",
      "tensor([[  5],\n",
      "        [ 20],\n",
      "        [ 46],\n",
      "        [ 52],\n",
      "        [ 79],\n",
      "        [ 83],\n",
      "        [ 90],\n",
      "        [ 95],\n",
      "        [ 99],\n",
      "        [106],\n",
      "        [110],\n",
      "        [112],\n",
      "        [118],\n",
      "        [129],\n",
      "        [136],\n",
      "        [138],\n",
      "        [141],\n",
      "        [152],\n",
      "        [170],\n",
      "        [177],\n",
      "        [187],\n",
      "        [238],\n",
      "        [241],\n",
      "        [254],\n",
      "        [265],\n",
      "        [283],\n",
      "        [289]])\n",
      "tensor([[ 20],\n",
      "        [ 22],\n",
      "        [ 34],\n",
      "        [ 47],\n",
      "        [ 65],\n",
      "        [ 80],\n",
      "        [ 86],\n",
      "        [103],\n",
      "        [112],\n",
      "        [117],\n",
      "        [120],\n",
      "        [124]])\n",
      "tensor([[ 22],\n",
      "        [ 26],\n",
      "        [ 32],\n",
      "        [ 84],\n",
      "        [ 93],\n",
      "        [ 98],\n",
      "        [132],\n",
      "        [137],\n",
      "        [140],\n",
      "        [152],\n",
      "        [156],\n",
      "        [180],\n",
      "        [195],\n",
      "        [203],\n",
      "        [207],\n",
      "        [221],\n",
      "        [258]])\n",
      "tensor([[  7],\n",
      "        [ 21],\n",
      "        [ 29],\n",
      "        [ 33],\n",
      "        [ 41],\n",
      "        [ 52],\n",
      "        [ 59],\n",
      "        [ 64],\n",
      "        [ 75],\n",
      "        [ 84],\n",
      "        [ 97],\n",
      "        [109],\n",
      "        [113],\n",
      "        [128],\n",
      "        [143],\n",
      "        [151],\n",
      "        [158],\n",
      "        [198],\n",
      "        [206],\n",
      "        [213],\n",
      "        [218],\n",
      "        [223],\n",
      "        [231],\n",
      "        [234],\n",
      "        [241],\n",
      "        [244],\n",
      "        [251],\n",
      "        [255],\n",
      "        [258],\n",
      "        [261],\n",
      "        [272],\n",
      "        [274],\n",
      "        [276],\n",
      "        [284],\n",
      "        [291],\n",
      "        [295],\n",
      "        [301]])\n",
      "tensor([[  8],\n",
      "        [ 11],\n",
      "        [ 18],\n",
      "        [ 25],\n",
      "        [ 29],\n",
      "        [ 37],\n",
      "        [ 48],\n",
      "        [ 76],\n",
      "        [ 82],\n",
      "        [ 91],\n",
      "        [103],\n",
      "        [108]])\n",
      "tensor([[ 18],\n",
      "        [ 26],\n",
      "        [ 31],\n",
      "        [ 42],\n",
      "        [ 48],\n",
      "        [ 68],\n",
      "        [ 81],\n",
      "        [ 86],\n",
      "        [ 92],\n",
      "        [103],\n",
      "        [110],\n",
      "        [118],\n",
      "        [124]])\n",
      "tensor([[17],\n",
      "        [33],\n",
      "        [38],\n",
      "        [57],\n",
      "        [64],\n",
      "        [68],\n",
      "        [73],\n",
      "        [78],\n",
      "        [92]])\n",
      "tensor([[ 20],\n",
      "        [ 23],\n",
      "        [ 43],\n",
      "        [ 58],\n",
      "        [ 65],\n",
      "        [ 84],\n",
      "        [ 88],\n",
      "        [ 92],\n",
      "        [ 95],\n",
      "        [105],\n",
      "        [116],\n",
      "        [126],\n",
      "        [133],\n",
      "        [191]])\n",
      "tensor([[  7],\n",
      "        [ 26],\n",
      "        [ 34],\n",
      "        [ 42],\n",
      "        [ 55],\n",
      "        [ 65],\n",
      "        [ 71],\n",
      "        [ 77],\n",
      "        [ 81],\n",
      "        [ 85],\n",
      "        [100],\n",
      "        [137],\n",
      "        [162],\n",
      "        [169],\n",
      "        [173],\n",
      "        [178],\n",
      "        [200],\n",
      "        [205],\n",
      "        [212],\n",
      "        [223],\n",
      "        [228],\n",
      "        [235],\n",
      "        [306],\n",
      "        [320],\n",
      "        [330],\n",
      "        [334],\n",
      "        [353],\n",
      "        [362],\n",
      "        [372],\n",
      "        [383],\n",
      "        [400],\n",
      "        [412],\n",
      "        [418],\n",
      "        [422],\n",
      "        [431],\n",
      "        [434],\n",
      "        [447],\n",
      "        [478],\n",
      "        [484],\n",
      "        [495],\n",
      "        [503],\n",
      "        [514],\n",
      "        [519],\n",
      "        [525]])\n",
      "tensor([[  7],\n",
      "        [ 12],\n",
      "        [ 15],\n",
      "        [ 22],\n",
      "        [ 27],\n",
      "        [ 40],\n",
      "        [ 48],\n",
      "        [ 59],\n",
      "        [ 61],\n",
      "        [ 73],\n",
      "        [ 78],\n",
      "        [ 92],\n",
      "        [110],\n",
      "        [121]])\n",
      "tensor([[ 69],\n",
      "        [ 75],\n",
      "        [ 93],\n",
      "        [102],\n",
      "        [109],\n",
      "        [112],\n",
      "        [122],\n",
      "        [125],\n",
      "        [147],\n",
      "        [151],\n",
      "        [163],\n",
      "        [171],\n",
      "        [179],\n",
      "        [185],\n",
      "        [189],\n",
      "        [201],\n",
      "        [205],\n",
      "        [207],\n",
      "        [210],\n",
      "        [212],\n",
      "        [227],\n",
      "        [230],\n",
      "        [247],\n",
      "        [290],\n",
      "        [294],\n",
      "        [304],\n",
      "        [310],\n",
      "        [324],\n",
      "        [333],\n",
      "        [338],\n",
      "        [347],\n",
      "        [350]])\n",
      "tensor([[ 24],\n",
      "        [ 30],\n",
      "        [ 35],\n",
      "        [ 37],\n",
      "        [ 41],\n",
      "        [ 53],\n",
      "        [ 61],\n",
      "        [ 67],\n",
      "        [ 70],\n",
      "        [ 80],\n",
      "        [ 84],\n",
      "        [ 94],\n",
      "        [102],\n",
      "        [109],\n",
      "        [119],\n",
      "        [126],\n",
      "        [135],\n",
      "        [138],\n",
      "        [141],\n",
      "        [153],\n",
      "        [156],\n",
      "        [158]])\n",
      "tensor([[ 10],\n",
      "        [ 24],\n",
      "        [ 31],\n",
      "        [ 37],\n",
      "        [ 39],\n",
      "        [ 61],\n",
      "        [ 65],\n",
      "        [ 74],\n",
      "        [ 79],\n",
      "        [ 94],\n",
      "        [106],\n",
      "        [120],\n",
      "        [124],\n",
      "        [145],\n",
      "        [157],\n",
      "        [184],\n",
      "        [189],\n",
      "        [194],\n",
      "        [201],\n",
      "        [203],\n",
      "        [210],\n",
      "        [223],\n",
      "        [234],\n",
      "        [238],\n",
      "        [242],\n",
      "        [265],\n",
      "        [274],\n",
      "        [287],\n",
      "        [313],\n",
      "        [319]])\n",
      "tensor([[  9],\n",
      "        [ 13],\n",
      "        [ 28],\n",
      "        [ 31],\n",
      "        [ 37],\n",
      "        [ 45],\n",
      "        [ 63],\n",
      "        [ 71],\n",
      "        [ 76],\n",
      "        [ 82],\n",
      "        [ 96],\n",
      "        [117],\n",
      "        [121],\n",
      "        [132],\n",
      "        [143],\n",
      "        [155],\n",
      "        [161],\n",
      "        [182],\n",
      "        [192],\n",
      "        [199],\n",
      "        [216],\n",
      "        [235],\n",
      "        [237],\n",
      "        [251],\n",
      "        [265],\n",
      "        [277],\n",
      "        [294],\n",
      "        [304],\n",
      "        [311],\n",
      "        [326],\n",
      "        [335]])\n",
      "tensor([[ 16],\n",
      "        [ 48],\n",
      "        [ 53],\n",
      "        [ 64],\n",
      "        [ 68],\n",
      "        [ 75],\n",
      "        [ 79],\n",
      "        [ 86],\n",
      "        [ 95],\n",
      "        [122],\n",
      "        [182],\n",
      "        [194],\n",
      "        [209],\n",
      "        [225],\n",
      "        [232],\n",
      "        [242],\n",
      "        [312],\n",
      "        [315],\n",
      "        [324],\n",
      "        [328],\n",
      "        [332],\n",
      "        [336]])\n",
      "tensor([[ 14],\n",
      "        [ 18],\n",
      "        [ 21],\n",
      "        [ 36],\n",
      "        [ 39],\n",
      "        [ 45],\n",
      "        [ 51],\n",
      "        [ 61],\n",
      "        [ 74],\n",
      "        [ 82],\n",
      "        [ 87],\n",
      "        [ 94],\n",
      "        [ 99],\n",
      "        [103],\n",
      "        [105],\n",
      "        [109],\n",
      "        [123],\n",
      "        [127],\n",
      "        [132],\n",
      "        [142],\n",
      "        [147],\n",
      "        [151],\n",
      "        [154],\n",
      "        [156],\n",
      "        [160],\n",
      "        [163],\n",
      "        [168],\n",
      "        [184],\n",
      "        [192],\n",
      "        [196],\n",
      "        [198],\n",
      "        [210],\n",
      "        [228]])\n",
      "tensor([[ 20],\n",
      "        [ 24],\n",
      "        [ 31],\n",
      "        [ 37],\n",
      "        [ 48],\n",
      "        [ 56],\n",
      "        [ 68],\n",
      "        [ 72],\n",
      "        [ 86],\n",
      "        [ 90],\n",
      "        [ 95],\n",
      "        [107],\n",
      "        [110],\n",
      "        [117],\n",
      "        [120],\n",
      "        [129],\n",
      "        [132],\n",
      "        [137],\n",
      "        [149],\n",
      "        [154],\n",
      "        [166]])\n",
      "tensor([[ 18],\n",
      "        [ 21],\n",
      "        [ 36],\n",
      "        [ 44],\n",
      "        [ 51],\n",
      "        [ 66],\n",
      "        [ 79],\n",
      "        [ 89],\n",
      "        [ 95],\n",
      "        [119],\n",
      "        [127],\n",
      "        [138],\n",
      "        [142],\n",
      "        [153],\n",
      "        [182],\n",
      "        [187],\n",
      "        [190],\n",
      "        [193],\n",
      "        [197]])\n",
      "tensor([[ 27],\n",
      "        [ 33],\n",
      "        [ 35],\n",
      "        [ 47],\n",
      "        [ 51],\n",
      "        [ 67],\n",
      "        [ 70],\n",
      "        [ 73],\n",
      "        [ 81],\n",
      "        [ 86],\n",
      "        [ 91],\n",
      "        [101],\n",
      "        [107],\n",
      "        [112],\n",
      "        [121],\n",
      "        [128],\n",
      "        [132],\n",
      "        [137],\n",
      "        [140],\n",
      "        [146]])\n",
      "tensor([[  5],\n",
      "        [ 36],\n",
      "        [ 39],\n",
      "        [ 67],\n",
      "        [ 78],\n",
      "        [ 99],\n",
      "        [113],\n",
      "        [144],\n",
      "        [170],\n",
      "        [176],\n",
      "        [186],\n",
      "        [189],\n",
      "        [196],\n",
      "        [209],\n",
      "        [217],\n",
      "        [225],\n",
      "        [228],\n",
      "        [240],\n",
      "        [247],\n",
      "        [253],\n",
      "        [276],\n",
      "        [284],\n",
      "        [287],\n",
      "        [290],\n",
      "        [298],\n",
      "        [301],\n",
      "        [306]])\n"
     ]
    }
   ],
   "source": [
    "arc_correct_inter, rel_correct_inter = eval_inter(inter_dataloader, masks_whole, root_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py395",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5 (default, May 18 2021, 14:42:02) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "1adbcecb1e586d9b8eb419680ab9509109c36838db5c3be5530ca5bad501e730"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
