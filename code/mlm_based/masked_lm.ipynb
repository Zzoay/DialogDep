{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05f863ea-b90a-41bb-b711-fe806a2f3c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from typing import *\n",
    "import json\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, IterableDataset\n",
    "from transformers import AutoTokenizer, AdamW, AutoModel, get_linear_schedule_with_warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fe9da27-36d0-4663-a71f-a4099056f987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from constant import rel2id, punct_lst, weak_signals, weak_labels\n",
    "from utils import to_cuda"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad37c83-79ad-41f8-b882-3a6be5b30ede",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5889476e-10bf-47cf-ab4c-a1b95e01015b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    data_file = '../aug/diag_codt/diag_train.conll'\n",
    "    # data_file = '../data_testset/1to800_1108.json'\n",
    "    plm = 'hfl/chinese-electra-180g-base-discriminator'\n",
    "    random_seed = 42\n",
    "    num_epochs = 1\n",
    "    batch_size = 32\n",
    "    plm_lr = 2e-5\n",
    "    head_lr = 1e-4\n",
    "    weight_decay = 0.1\n",
    "    dropout = 0.1\n",
    "    grad_clip = 1\n",
    "    scheduler = 'linear'\n",
    "    warmup_ratio = 0.1\n",
    "    num_early_stop = 3\n",
    "    max_length = 128\n",
    "    hidden_size = 400\n",
    "    num_labels = 35\n",
    "    print_every = 100\n",
    "    save_every = 200\n",
    "    eval_every = 1e9\n",
    "    cuda = True\n",
    "    fp16 = True\n",
    "    debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecce4b-c209-4404-84d8-67c9c26c4fb4",
   "metadata": {},
   "source": [
    "# Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85317be3-f17f-495f-8cca-db405577d6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=CFG.random_seed):\n",
    "    np.random.seed(seed%(2**32-1))\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic =True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6ca258-7edc-4bb9-81bc-22fd030ddef4",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b841c3b-0613-4ac0-8592-31f72ae69982",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_signals = {\n",
    "                # '但': 36, '但是': 36, \n",
    "                '情况':36, '为什么': 36,\n",
    "                '吗': 37, '?': 37, '什么': 37,\n",
    "                # '呢': 37, '吧': 37,\n",
    "                '请': 39, '麻烦':39, '希望': 39, '让': 39, '咨询': 39}     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f0dd6e1e-96b9-4903-87a9-7dcb4fe0418e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'说': 21, '表示': 21, '看到': 21, '显示': 21, '知道': 21, '认为': 21, '希望': 21, '指出': 21, '如果': 25, '假如': 25, '的话': 25, '若': 25, '如': 25, '要是': 25, '倘若': 25, '因为': 23, '所以': 23, '导致': 23, '因此': 23, '造成': 23, '由于': 23, '因而': 23, '但是': 26, '可是': 26, '但': 26, '竟': 26, '却': 26, '不过': 26, '居然': 26, '而是': 26, '反而': 26, '以及': 31, '也': 31, '并': 31, '并且': 31, '又': 31, '或者': 31, '对于': 22, '自从': 22, '上次': 22, '明天': 34, '晚上': 34, '到时候': 34, '再': 34, '然后': 34, '接下来': 34, '最后': 34, '随后': 34, '为了': 28, '使': 28, '为 的': 28, '为 了': 28, '通过': 32, '必须': 32, '点击': 32, '对 吗': 33, '是 吗': 33, '对 吧': 33, '是 吧': 33, '对 ?': 33, '别 的': 24, '另外': 24, '解释': 30, '比如': 30, '例如': 30, '是 这样': 30, '理想': 29, '真 棒': 29, '太 棒': 29, '真差': 29, '太 差': 29, '不 行': 29, '扯皮': 29, '这么 麻烦': 29} 74\n",
      "{'说': 21, '表示': 21, '看到': 21, '显示': 21, '知道': 21, '认为': 21, '希望': 39, '指出': 21, '如果': 25, '假如': 25, '的话': 25, '若': 25, '如': 25, '要是': 25, '倘若': 25, '因为': 23, '所以': 23, '导致': 23, '因此': 23, '造成': 23, '由于': 23, '因而': 23, '但是': 26, '可是': 26, '但': 26, '竟': 26, '却': 26, '不过': 26, '居然': 26, '而是': 26, '反而': 26, '以及': 31, '也': 31, '并': 31, '并且': 31, '又': 31, '或者': 31, '对于': 22, '自从': 22, '上次': 22, '明天': 34, '晚上': 34, '到时候': 34, '再': 34, '然后': 34, '接下来': 34, '最后': 34, '随后': 34, '为了': 28, '使': 28, '为 的': 28, '为 了': 28, '通过': 32, '必须': 32, '点击': 32, '对 吗': 33, '是 吗': 33, '对 吧': 33, '是 吧': 33, '对 ?': 33, '别 的': 24, '另外': 24, '解释': 30, '比如': 30, '例如': 30, '是 这样': 30, '理想': 29, '真 棒': 29, '太 棒': 29, '真差': 29, '太 差': 29, '不 行': 29, '扯皮': 29, '这么 麻烦': 29, '情况': 36, '为什么': 36, '吗': 37, '?': 37, '什么': 37, '请': 39, '麻烦': 39, '让': 39, '咨询': 39} 83\n"
     ]
    }
   ],
   "source": [
    "origin4change = [rel2id[x] for x in ['root', 'dfsubj', 'sasubj']]\n",
    "# origin4change.extend([i for i in range(21, 35)])\n",
    "\n",
    "signal_dct = {}\n",
    "for i, signals in enumerate(weak_signals):\n",
    "    for s in signals:\n",
    "        signal_dct[s] = weak_labels[i]\n",
    "print(signal_dct, len(signal_dct.keys()))\n",
    "signal_dct.update(inter_signals)\n",
    "print(signal_dct, len(signal_dct.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe3103d-66cd-4cd7-bafb-c6a8d2db6c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dependency():\n",
    "    def __init__(self, idx, word, head, rel):\n",
    "        self.id = idx\n",
    "        self.word = word\n",
    "        self.head = head\n",
    "        self.rel = rel\n",
    "\n",
    "    def __str__(self):\n",
    "        # example:  1\t上海\t_\tNR\tNR\t_\t2\tnn\t_\t_\n",
    "        values = [str(self.idx), self.word, \"_\", \"_\", \"_\", \"_\", str(self.head), self.rel, \"_\", \"_\"]\n",
    "        return '\\t'.join(values)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"({self.word}, {self.head}, {self.rel})\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3521966d-6221-4354-a796-21aca80cb9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_lst, sample = [], []\n",
    "with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        toks = line.strip().split('\\t')\n",
    "        if len(toks) == 1:\n",
    "            sample_lst.append(sample)\n",
    "            sample = []\n",
    "            continue\n",
    "        \n",
    "        sample.append(Dependency(int(toks[0]), toks[1], toks[6], toks[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4899d5cd-760e-439d-9245-ca4cf0b09bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_lst = []\n",
    "# with open(CFG.data_file, 'r', encoding='utf-8') as f:\n",
    "#     data = json.load(f)\n",
    "# for d in data:\n",
    "#     rel_dct = {}\n",
    "#     for tripple in d['relationship']:\n",
    "#         head, rel, tail = tripple\n",
    "#         head_uttr_idx, head_word_idx = [int(x) for x in head.split('-')]\n",
    "#         tail_uttr_idx, tail_word_idx = [int(x) for x in tail.split('-')]\n",
    "#         if head_uttr_idx != tail_uttr_idx:\n",
    "#             continue\n",
    "\n",
    "#         if not rel_dct.get(head_uttr_idx, None):\n",
    "#             rel_dct[head_uttr_idx] = {tail_word_idx: [head_word_idx, rel]}\n",
    "#         else:\n",
    "#             rel_dct[head_uttr_idx][tail_word_idx] = [head_word_idx, rel]\n",
    "            \n",
    "#     for item in d['dialog']:\n",
    "#         turn = item['turn']\n",
    "#         utterance = item['utterance']\n",
    "#         # dep_lst:List[Dependency] = [Dependency(0, '[root]', -1, '_')]\n",
    "#         sample:List[Dependency] = []\n",
    "\n",
    "#         for word_idx, word in enumerate(utterance.split(' ')):\n",
    "#             head_word_idx, rel = rel_dct[turn].get(word_idx + 1, [word_idx, 'adjct'])  # some word annoted missed, padded with last word and 'adjct'\n",
    "#             sample.append(Dependency(word_idx + 1, word, head_word_idx, rel))  # start from 1\n",
    "\n",
    "#         sample_lst.append(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d94ddb7-c2b5-413f-aea8-c86c79a9141e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 236590/236590 [00:04<00:00, 53804.46it/s]\n"
     ]
    }
   ],
   "source": [
    "labels = []\n",
    "edu_lst = []\n",
    "for sample in tqdm(sample_lst):\n",
    "    edu, label = [], []\n",
    "    for i, dep in enumerate(sample[1:]):\n",
    "        word = dep.word\n",
    "        rel = dep.rel\n",
    "        \n",
    "        edu.append(dep)\n",
    "        \n",
    "        if word in signal_dct.keys():\n",
    "            label.append(word)\n",
    "            if signal_dct[f'{word}'] not in [21, 29, 33] and random.random() < 0.7:\n",
    "                edu.pop(-1)\n",
    "        elif f'{sample[i-1].word} {word}' in signal_dct.keys():\n",
    "            popped = False\n",
    "            if sample[i-1].word in signal_dct.keys():\n",
    "                label.pop(-1)\n",
    "                if sample[i-1].word != edu[-1].word:  # last word have dropped\n",
    "                    edu.pop(-1)\n",
    "                    popped = True\n",
    "            label.append(f'{sample[i-1].word}{word}')\n",
    "            if ~popped and signal_dct[f'{sample[i-1].word} {word}'] not in [21, 29, 33] and random.random() < 0.7:  # drop\n",
    "                edu.pop(-1)\n",
    "                if sample[i-1].word in signal_dct.keys() and sample[i-1].word == edu[-1].word:  # not pop before\n",
    "                    edu.pop(-1)\n",
    "        elif random.random() < 0.2: # drop\n",
    "            edu.pop(-1)\n",
    "        \n",
    "        # 'edu'\n",
    "        if word in punct_lst:\n",
    "            # if len(label) == 0 and random.random() < 0.2:  # drop\n",
    "            #     edu, label = [], []\n",
    "            #     continue\n",
    "            edu_lst.append(edu)\n",
    "            labels.append(label)                                \n",
    "            edu, label = [], []\n",
    "    \n",
    "    if len(sample) > 1:\n",
    "        edu.append(sample[-1])\n",
    "        if sample[-1].word in signal_dct.keys():\n",
    "            label.append(word)\n",
    "    \n",
    "    # if len(label) == 0 and random.random() < 0.2:\n",
    "    #     continue\n",
    "        \n",
    "    edu_lst.append(edu)\n",
    "    labels.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7072713e-0caa-41b1-8063-ccfe776063c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "115136 267022\n"
     ]
    }
   ],
   "source": [
    "pos_cnt, neg_cnt = 0, 0\n",
    "for label in labels:\n",
    "    if len(label) != 0:\n",
    "        pos_cnt += 1\n",
    "    else:\n",
    "        neg_cnt += 1\n",
    "        \n",
    "print(pos_cnt, neg_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82f0c554-2275-46d7-81b8-1826dabdf25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21128\n",
      "EOS: 100\n",
      "add token: [root] 21128\n",
      "add token: [qst] 21129\n",
      "add token: [ans] 21130\n",
      "add token: [none] 21131\n",
      "21132\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(CFG.plm)\n",
    "print(len(tokenizer))\n",
    "\n",
    "num_added_toks = tokenizer.add_tokens(['[root]', '[qst]', '[ans]', '[none]'], special_tokens=True)\n",
    "\n",
    "tokenizer.eos_token = '[EOS]'\n",
    "print(f'EOS: {tokenizer.eos_token_id}')\n",
    "\n",
    "tokenizer.root_token = '[root]'\n",
    "tokenizer.root_token_ids = tokenizer('[root]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.root_token} {tokenizer.root_token_ids}\")\n",
    "\n",
    "tokenizer.qst_token = '[qst]'\n",
    "tokenizer.qst_token_ids = tokenizer('[qst]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.qst_token} {tokenizer.qst_token_ids}\")\n",
    "\n",
    "tokenizer.ans_token = '[ans]'\n",
    "tokenizer.ans_token_ids = tokenizer('[ans]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.ans_token} {tokenizer.ans_token_ids}\")\n",
    "\n",
    "tokenizer.none_token = '[none]'\n",
    "tokenizer.none_token_ids = tokenizer('[none]')['input_ids'][1]\n",
    "print(f\"add token: {tokenizer.none_token} {tokenizer.none_token_ids}\")\n",
    "\n",
    "# tokenizer.signal_token = '[signal]'\n",
    "# tokenizer.signal_token_ids = tokenizer('[signal]')['input_ids'][1]\n",
    "# print(f\"add token: {tokenizer.signal_token} {tokenizer.signal_token_ids}\")\n",
    "\n",
    "print(len(tokenizer))\n",
    "CFG.tokenizer = tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f82fa404-08c0-4a22-bf49-098cade51b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'说': [6432], '表示': [6134, 4850], '看到': [4692, 1168], '显示': [3227, 4850], '知道': [4761, 6887], '认为': [6371, 711], '希望': [2361, 3307], '指出': [2900, 1139], '如果': [1963, 3362], '假如': [969, 1963], '的话': [4638, 6413], '若': [5735], '如': [1963], '要是': [6206, 3221], '倘若': [951, 5735], '因为': [1728, 711], '所以': [2792, 809], '导致': [2193, 5636], '因此': [1728, 3634], '造成': [6863, 2768], '由于': [4507, 754], '因而': [1728, 5445], '但是': [852, 3221], '可是': [1377, 3221], '但': [852], '竟': [4994], '却': [1316], '不过': [679, 6814], '居然': [2233, 4197], '而是': [5445, 3221], '反而': [1353, 5445], '以及': [809, 1350], '也': [738], '并': [2400], '并且': [2400, 684], '又': [1348], '或者': [2772, 5442], '对于': [2190, 754], '自从': [5632, 794], '上次': [677, 3613], '明天': [3209, 1921], '晚上': [3241, 677], '到时候': [1168, 3198, 952], '再': [1086], '然后': [4197, 1400], '接下来': [2970, 678, 3341], '最后': [3297, 1400], '随后': [7390, 1400], '为了': [711, 749], '使': [886], '为的': [711, 4638], '通过': [6858, 6814], '必须': [2553, 7557], '点击': [4157, 1140], '对吗': [2190, 1408], '是吗': [3221, 1408], '对吧': [2190, 1416], '是吧': [3221, 1416], '对?': [2190, 136], '别的': [1166, 4638], '另外': [1369, 1912], '解释': [6237, 7025], '比如': [3683, 1963], '例如': [891, 1963], '是这样': [3221, 6821, 3416], '理想': [4415, 2682], '真棒': [4696, 3472], '太棒': [1922, 3472], '真差': [4696, 2345], '太差': [1922, 2345], '不行': [679, 6121], '扯皮': [2816, 4649], '这么麻烦': [6821, 720, 7937, 4172], '情况': [2658, 1105], '为什么': [711, 784, 720], '吗': [1408], '?': [136], '什么': [784, 720], '请': [6435], '麻烦': [7937, 4172], '让': [6375], '咨询': [1486, 6418]}\n"
     ]
    }
   ],
   "source": [
    "label2id = {}\n",
    "for l in signal_dct.keys():\n",
    "    label_word = ''.join(l.split(' '))\n",
    "    label2id[label_word] = CFG.tokenizer(label_word)['input_ids'][1:-1]\n",
    "print(label2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "391cdd45-b0c7-40b5-bc1d-eeb093d17372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EduDataset(Dataset):\n",
    "#     def __init__(self, cfg, train, prompt, edu_lst, labels):\n",
    "#         self.train = train\n",
    "#         self.cfg = cfg\n",
    "#         self.prompt = prompt\n",
    "#         self.inputs, self.heads, self.rels, self.labels, self.masks = self.read_data(edu_lst, labels)\n",
    "        \n",
    "#     def read_data(self, edu_lst, labels):\n",
    "#         inputs, offsets = [], []\n",
    "#         tags, heads, rels, masks = [], [], [], []\n",
    "        \n",
    "#         ans_word_len = 3\n",
    "#         answer_words = []\n",
    "        \n",
    "#         for deps, label in tqdm(zip(edu_lst, labels)):\n",
    "#             seq_len = len(deps)\n",
    "\n",
    "#             word_lst = [] \n",
    "#             rel_attr = {'input_ids':torch.Tensor(), 'token_type_ids':torch.Tensor(), 'attention_mask':torch.Tensor()}\n",
    "#             head_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)  # same as root index is 0, constrainting by mask \n",
    "#             rel_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "#             mask_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "#             ans_tokens = torch.zeros((ans_word_len, len(self.cfg.tokenizer)))\n",
    "#             for i, dep in enumerate(deps):\n",
    "#                 if i == seq_len or i + 1== self.cfg.max_length:\n",
    "#                     break\n",
    "\n",
    "#                 word_lst.append(dep.word)\n",
    "\n",
    "#                 if dep.head in ['_', '-1'] or int(dep.head) + 1 >= self.cfg.max_length:\n",
    "#                     head_tokens[i+1] = 0\n",
    "#                     mask_tokens[i+1] = 0\n",
    "#                 else:\n",
    "#                     head_tokens[i+1] = int(dep.head)\n",
    "#                     mask_tokens[i+1] = 1\n",
    "\n",
    "#                 if self.train:\n",
    "#                     rel_tokens[i+1] = rel2id[dep.rel]\n",
    "#                 else:\n",
    "#                     rel_tokens[i+1] = rel2id.get(dep.rel, rel2id['adjct'])\n",
    "\n",
    "#             word_lst = self.prompt.split(' ') + ['[SEP]'] + word_lst\n",
    "            \n",
    "#             tokenized = self.cfg.tokenizer.encode_plus(word_lst, \n",
    "#                                               padding='max_length', \n",
    "#                                               truncation=True,\n",
    "#                                               max_length=self.cfg.max_length, \n",
    "#                                               return_offsets_mapping=False, \n",
    "#                                               return_tensors='pt',\n",
    "#                                               is_split_into_words=True)\n",
    "#             inputs.append({\"input_ids\": tokenized['input_ids'][0],\n",
    "#                           \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "#                            \"attention_mask\": tokenized['attention_mask'][0]\n",
    "#                           })\n",
    "            \n",
    "#             if len(label) == 0:\n",
    "#                 ans_tokens[0:ans_word_len, self.cfg.tokenizer.none_token_ids] = 1\n",
    "#             else: \n",
    "#                 # label_id = self.cfg.tokenizer(l)['input_ids'][1:-1]\n",
    "#                 label_id = label2id[label[0]]  # first label\n",
    "#                 if len(label_id) > ans_word_len:\n",
    "#                     label_id = label_id[:ans_word_len]\n",
    "#                 ans_tokens[torch.arange(len(label_id)), label_id] = 1\n",
    "#                 if len(label_id) < ans_word_len:\n",
    "#                     ans_tokens[torch.arange(len(label_id), ans_word_len), self.cfg.tokenizer.eos_token_id] = 1  # padding\n",
    "            \n",
    "#             answer_words.append(ans_tokens)\n",
    "\n",
    "#             heads.append(head_tokens)\n",
    "#             rels.append(rel_tokens)\n",
    "#             masks.append(mask_tokens)\n",
    "                    \n",
    "#         return inputs, heads, rels, answer_words, masks\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         return self.inputs[idx], self.heads[idx], self.rels[idx], self.labels[idx], self.masks[idx]\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return len(self.rels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14359551-4835-466c-81a5-67efc9ad6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EduDataset(IterableDataset):\n",
    "    def __init__(self, cfg, train, prompt, edu_lst, labels):\n",
    "        self.train = train\n",
    "        self.cfg = cfg\n",
    "        self.prompt = prompt\n",
    "        self.edu_lst = edu_lst\n",
    "        self.labels = labels\n",
    "        \n",
    "    def read_data(self, edu_lst, labels):\n",
    "        inputs, offsets = [], []\n",
    "        tags, heads, rels, masks = [], [], [], []\n",
    "        \n",
    "        ans_word_len = 3\n",
    "        answer_words = []\n",
    "        \n",
    "        for deps, label in zip(edu_lst, labels):\n",
    "            seq_len = len(deps)\n",
    "\n",
    "            word_lst = [] \n",
    "            rel_attr = {'input_ids':torch.Tensor(), 'token_type_ids':torch.Tensor(), 'attention_mask':torch.Tensor()}\n",
    "            head_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)  # same as root index is 0, constrainting by mask \n",
    "            rel_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            mask_tokens = np.zeros(self.cfg.max_length, dtype=np.int64)\n",
    "            ans_tokens = torch.zeros((ans_word_len, len(self.cfg.tokenizer)))\n",
    "            for i, dep in enumerate(deps):\n",
    "                if i == seq_len or i + 1== self.cfg.max_length:\n",
    "                    break\n",
    "\n",
    "                word_lst.append(dep.word)\n",
    "\n",
    "                if dep.head in ['_', '-1'] or int(dep.head) + 1 >= self.cfg.max_length:\n",
    "                    head_tokens[i+1] = 0\n",
    "                    mask_tokens[i+1] = 0\n",
    "                else:\n",
    "                    head_tokens[i+1] = int(dep.head)\n",
    "                    mask_tokens[i+1] = 1\n",
    "\n",
    "                if self.train:\n",
    "                    rel_tokens[i+1] = rel2id[dep.rel]\n",
    "                else:\n",
    "                    rel_tokens[i+1] = rel2id.get(dep.rel, rel2id['adjct'])\n",
    "\n",
    "            word_lst = self.prompt.split(' ') + ['[SEP]'] + word_lst\n",
    "            \n",
    "            tokenized = self.cfg.tokenizer.encode_plus(word_lst, \n",
    "                                              padding='max_length', \n",
    "                                              truncation=True,\n",
    "                                              max_length=self.cfg.max_length, \n",
    "                                              return_offsets_mapping=False, \n",
    "                                              return_tensors='pt',\n",
    "                                              is_split_into_words=True)\n",
    "            tokenized = {\"input_ids\": tokenized['input_ids'][0],\n",
    "                          \"token_type_ids\": tokenized['token_type_ids'][0],\n",
    "                           \"attention_mask\": tokenized['attention_mask'][0]\n",
    "                          }\n",
    "            \n",
    "            if len(label) == 0:\n",
    "                ans_tokens[0:ans_word_len, self.cfg.tokenizer.none_token_ids] = 1\n",
    "            else: \n",
    "                # label_id = self.cfg.tokenizer(l)['input_ids'][1:-1]\n",
    "                label_id = label2id[label[0]]  # first label\n",
    "                if len(label_id) > ans_word_len:\n",
    "                    label_id = label_id[:ans_word_len]\n",
    "                ans_tokens[torch.arange(len(label_id)), label_id] = 1\n",
    "                if len(label_id) < ans_word_len:\n",
    "                    ans_tokens[torch.arange(len(label_id), ans_word_len), self.cfg.tokenizer.eos_token_id] = 1  # padding\n",
    "                \n",
    "                for l in enumerate(label):\n",
    "                    if l not in inter_signals.keys():\n",
    "                        continue\n",
    "                    label_id = label2id[label[0]]  # first label\n",
    "                    if len(label_id) > ans_word_len:\n",
    "                        label_id = label_id[:ans_word_len]\n",
    "                    ans_tokens[torch.arange(len(label_id)), label_id] = 1\n",
    "                    if len(label_id) < ans_word_len:\n",
    "                        ans_tokens[torch.arange(len(label_id), ans_word_len), self.cfg.tokenizer.eos_token_id] = 1  # padding    \n",
    "                        \n",
    "                    if l != label[0]:  # two different label\n",
    "                        ans_tokens[torch.arange(len(label_id), ans_word_len), :] /= 2\n",
    "                    break\n",
    "            \n",
    "#             answer_words.append(ans_tokens)\n",
    "\n",
    "#             heads.append(head_tokens)\n",
    "#             rels.append(rel_tokens)\n",
    "#             masks.append(mask_tokens)\n",
    "                    \n",
    "            yield tokenized, head_tokens, rel_tokens, ans_tokens, mask_tokens\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self.read_data(self.edu_lst, self.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed360646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 101, 5543, 6134, 4385, 5063, 4995, 6427,  721,  928, 1384, 4638, 6404,\n",
      "         3221, 8038,  103,  103,  103,  102]])\n",
      "[14, 15, 16]\n"
     ]
    }
   ],
   "source": [
    "prompt = '能 表现 篇章 语义 信号 的 词 是 ： [MASK] [MASK] [MASK]'\n",
    "prompt_tokenized = CFG.tokenizer.encode_plus(prompt.split(' '), is_split_into_words=True, return_tensors='pt')\n",
    "print(prompt_tokenized['input_ids'])\n",
    "\n",
    "masked_idx = [x[1].item() for x in (prompt_tokenized['input_ids'] == CFG.tokenizer.mask_token_id).nonzero()]\n",
    "CFG.masked_idx = masked_idx\n",
    "print(masked_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8ec821da-5e8a-45b7-bbdd-cce5472d56e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CFG.debug:\n",
    "    dataset = EduDataset(CFG, True, prompt, edu_lst[:3000], labels[:3000])\n",
    "else:\n",
    "    dataset = EduDataset(CFG, True, prompt, edu_lst, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87bde112-032a-4227-b358-3fa552e86ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logit = torch.randn(3, len(CFG.tokenizer))\n",
    "# gt = dataset[0][-2]\n",
    "\n",
    "# print(logit.shape)\n",
    "# print(gt.shape)\n",
    "\n",
    "# F.cross_entropy(logit, gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2cdba37",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df37d793",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "    \n",
    "        self.encoder = AutoModel.from_pretrained(cfg.plm)\n",
    "        self.encoder.resize_token_embeddings(len(cfg.tokenizer))\n",
    "        \n",
    "        # for param in self.encoder.parameters():\n",
    "        #     param.requires_grad = False\n",
    "\n",
    "        self.mlm_head = nn.Linear(self.encoder.config.hidden_size, len(cfg.tokenizer))\n",
    "\n",
    "        self.dropout = nn.Dropout(cfg.dropout)\n",
    "\n",
    "    def forward(self, inputs, labels=None):\n",
    "        feats, *_ = self.encoder(**inputs, return_dict=False)   # batch_size, seq_len (tokenized), plm_hidden_size\n",
    "        feats = self.dropout(feats)\n",
    "\n",
    "        masked_feats = feats[:, self.cfg.masked_idx, :]   # batch_size, masked_len, plm_hidden_size\n",
    "\n",
    "        logit = self.mlm_head(masked_feats)  # batch_size, masked_len, vocab_size\n",
    "\n",
    "        if labels is not None: # labels: [batch_size, masked_len, vocab_size]\n",
    "            loss = F.cross_entropy(logit.view(-1, labels.size()[-1]), labels.view(-1, labels.size()[-1]), reduction='none')\n",
    "            loss = loss.view(labels.size()[0], labels.size()[1])\n",
    "            loss = loss.sum(1).mean()\n",
    "            return logit, loss\n",
    "        return logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552e85f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at hfl/chinese-electra-180g-base-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = PromptModel(CFG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ce05e8",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3b6eee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer():\n",
    "    def __init__(self, \n",
    "                 model,\n",
    "                 trainset_size,\n",
    "                 config: Dict) -> None:\n",
    "\n",
    "        plm_params = [p for n,p in model.named_parameters() if 'encoder' in n]\n",
    "        head_params = [p for n,p in model.named_parameters() if 'encoder' not in n]\n",
    "        self.optim = AdamW([{'params': plm_params, 'lr':config.plm_lr}, \n",
    "                            {'params': head_params, 'lr':config.head_lr}], \n",
    "                            lr=config.plm_lr,\n",
    "                            weight_decay=config.weight_decay\n",
    "                          )\n",
    "        \n",
    "        training_step = int(config.num_epochs * (trainset_size / config.batch_size))\n",
    "        warmup_step = int(config.warmup_ratio * training_step)  \n",
    "        self.optim_schedule = get_linear_schedule_with_warmup(optimizer=self.optim, \n",
    "                                                              num_warmup_steps=warmup_step, \n",
    "                                                              num_training_steps=training_step)\n",
    "        self.scaler = torch.cuda.amp.GradScaler(enabled=config.fp16)\n",
    "\n",
    "        self.config = config\n",
    "\n",
    "    def train(self, \n",
    "              model: nn.Module, \n",
    "              train_iter: DataLoader, \n",
    "              val_iter: DataLoader):\n",
    "        model.train()\n",
    "        if self.config.cuda and torch.cuda.is_available():\n",
    "            model.cuda()\n",
    "            pass\n",
    "        \n",
    "        best_res = [1e4, 0, 0]\n",
    "        early_stop_cnt = 0\n",
    "        best_state_dict = None\n",
    "        step = 0\n",
    "        for epoch in tqdm(range(self.config.num_epochs)):\n",
    "            for batch in train_iter:\n",
    "                inputs, heads, rels, labels, masks = batch\n",
    "                \n",
    "                if self.config.cuda and torch.cuda.is_available():\n",
    "                    inputs_cuda = {}\n",
    "                    for key, value in inputs.items():\n",
    "                        inputs_cuda[key] = value.cuda()\n",
    "                    inputs = inputs_cuda\n",
    "                    \n",
    "                    heads, rels, labels, masks = to_cuda(data=(heads, rels, labels, masks))\n",
    "                \n",
    "                logit, loss = model(inputs, labels)\n",
    "                \n",
    "                self.optim.zero_grad()\n",
    "                if self.config.cuda and self.config.fp16:\n",
    "                    self.scaler.scale(loss).backward()\n",
    "                    self.scaler.unscale_(self.optim)\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), max_norm=self.config.grad_clip)\n",
    "\n",
    "                if self.config.fp16:\n",
    "                    self.scaler.step(self.optim)\n",
    "                    self.scaler.update()\n",
    "                else:\n",
    "                    self.optim.step()\n",
    "                self.optim_schedule.step()\n",
    "\n",
    "                # metrics = self.metrics_fn(arc_logits, rel_logits, heads, rels, masks)\n",
    "\n",
    "                if (step) % self.config.print_every == 0:\n",
    "                    print(f\"--epoch {epoch}, step {step}, loss {loss}\")\n",
    "                    # print(f\"  {metrics}\")\n",
    "                    \n",
    "                if (step + 1) % self.config.save_every == 0:\n",
    "                    print(f'-------saving---------')\n",
    "                    torch.save(model.state_dict(), f\"../results/prompt_model_diag.pt\")\n",
    "                    # print(f\"  {metrics}\")\n",
    "\n",
    "                if val_iter and (step) % self.config.eval_every == 0:\n",
    "                    avg_loss = self.eval(model, val_iter)\n",
    "                    res = [avg_loss]\n",
    "                    if loss < best_res[0]:  # las\n",
    "                        best_res = res\n",
    "                        best_state_dict = model.state_dict()\n",
    "                        torch.save(best_state_dict, f\"../results/prompt_model_diag.pt\")\n",
    "                        early_stop_cnt = 0\n",
    "                    else:\n",
    "                        early_stop_cnt += 1\n",
    "                    \n",
    "                    print(\"--Best Evaluation: \")\n",
    "                    print(\"-Loss: {}\\n\".format(*best_res))\n",
    "                    # back to train mode\n",
    "                    model.train()\n",
    "                \n",
    "                if early_stop_cnt >= self.config.num_early_stop:\n",
    "                    print(\"--early stopping, training finished.\")\n",
    "                    return best_res, best_state_dict\n",
    "\n",
    "                step += 1\n",
    "        \n",
    "        print(\"--training finished.\")\n",
    "        \n",
    "        if best_state_dict is not None:\n",
    "            return best_res, best_state_dict\n",
    "        \n",
    "        return best_res, model.state_dict()\n",
    "\n",
    "    # eval func\n",
    "    def eval(self, model: nn.Module, eval_iter: DataLoader, save_file: str = \"\", save_title: str = \"\"):\n",
    "        model.eval()\n",
    "\n",
    "        head_whole, rel_whole, mask_whole = torch.Tensor(), torch.Tensor(), torch.Tensor()\n",
    "        logit_whole = torch.Tensor()\n",
    "        avg_loss = 0.0\n",
    "        for step, batch in enumerate(eval_iter):\n",
    "            inputs, heads, rels, labels, masks = batch\n",
    "\n",
    "            if self.config.cuda and torch.cuda.is_available():\n",
    "                inputs_cuda = {}\n",
    "                for key, value in inputs.items():\n",
    "                    inputs_cuda[key] = value.cuda()\n",
    "                inputs = inputs_cuda\n",
    "\n",
    "                heads, rels, labels, masks = to_cuda(data=(heads, rels, labels, masks))\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                logit, loss = model(inputs, labels)\n",
    "\n",
    "            logit_whole = torch.cat([logit_whole, logit.cpu()], dim=0)\n",
    "\n",
    "            head_whole, rel_whole = torch.cat([head_whole, heads.cpu()], dim=0), torch.cat([rel_whole, rels.cpu()], dim=0)\n",
    "            mask_whole = torch.cat([mask_whole, masks.cpu()], dim=0)\n",
    "\n",
    "            avg_loss += loss.item() * len(heads)  # times the batch size of data\n",
    "\n",
    "\n",
    "        avg_loss /= len(eval_iter.dataset)  # type: ignore\n",
    "\n",
    "        print(\"--Evaluation:\")\n",
    "        print(\"Avg Loss: {}  \\n\".format(avg_loss))\n",
    "\n",
    "        if save_file != \"\":\n",
    "            results = [save_title, avg_loss, uas, las]  # type: ignore\n",
    "            results = [str(x) for x in results]\n",
    "            with open(save_file, \"a+\") as f:\n",
    "                f.write(\",\".join(results) + \"\\n\")  # type: ignore\n",
    "\n",
    "        return avg_loss\n",
    "    \n",
    "    def save_results(self, save_file, save_title, results):\n",
    "        saves = [save_title] + results\n",
    "        saves = [str(x) for x in saves]\n",
    "        with open(save_file, \"a+\") as f:\n",
    "            f.write(\",\".join(saves) + \"\\n\")  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bba99fc",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7a43b637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_ids = list(range(len(dataset)))\n",
    "# random.shuffle(full_ids)\n",
    "\n",
    "# ratio = 0.0\n",
    "# trainset_size = int(len(dataset) * (1 - ratio))\n",
    "# train_ids = full_ids[:trainset_size]\n",
    "# val_ids = full_ids[trainset_size:]\n",
    "\n",
    "# train_dataset = Subset(dataset, train_ids)\n",
    "# val_dataset = Subset(dataset, val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b10da4ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = DataLoader(dataset, batch_size=CFG.batch_size)\n",
    "val_iter = DataLoader(dataset, batch_size=CFG.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a43ab66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--epoch 0, step 0, loss 30.07476043701172\n",
      "--epoch 0, step 100, loss 17.39851188659668\n",
      "-------saving---------\n",
      "--epoch 0, step 200, loss 5.753630638122559\n",
      "--epoch 0, step 300, loss 2.9933180809020996\n",
      "-------saving---------\n",
      "--epoch 0, step 400, loss 1.7638427019119263\n",
      "--epoch 0, step 500, loss 2.4559969902038574\n",
      "-------saving---------\n",
      "--epoch 0, step 600, loss 1.8246821165084839\n",
      "--epoch 0, step 700, loss 1.4699232578277588\n",
      "-------saving---------\n",
      "--epoch 0, step 800, loss 1.3855056762695312\n",
      "--epoch 0, step 900, loss 1.147589087486267\n",
      "-------saving---------\n",
      "--epoch 0, step 1000, loss 1.1596451997756958\n",
      "--epoch 0, step 1100, loss 1.016486406326294\n",
      "-------saving---------\n",
      "--epoch 0, step 1200, loss 1.1765527725219727\n",
      "--epoch 0, step 1300, loss 0.6073367595672607\n",
      "-------saving---------\n",
      "--epoch 0, step 1400, loss 0.5795507431030273\n",
      "--epoch 0, step 1500, loss 1.0257056951522827\n",
      "-------saving---------\n",
      "--epoch 0, step 1600, loss 0.6995720267295837\n",
      "--epoch 0, step 1700, loss 0.5993667840957642\n",
      "-------saving---------\n",
      "--epoch 0, step 1800, loss 0.6968111991882324\n",
      "--epoch 0, step 1900, loss 0.8691724538803101\n",
      "-------saving---------\n",
      "--epoch 0, step 2000, loss 1.0849262475967407\n",
      "--epoch 0, step 2100, loss 0.8714452385902405\n",
      "-------saving---------\n",
      "--epoch 0, step 2200, loss 1.181839942932129\n",
      "--epoch 0, step 2300, loss 1.3655081987380981\n",
      "-------saving---------\n",
      "--epoch 0, step 2400, loss 0.8978390097618103\n",
      "--epoch 0, step 2500, loss 0.9035233855247498\n",
      "-------saving---------\n",
      "--epoch 0, step 2600, loss 0.841378927230835\n",
      "--epoch 0, step 2700, loss 1.0078606605529785\n",
      "-------saving---------\n",
      "--epoch 0, step 2800, loss 0.7568389177322388\n",
      "--epoch 0, step 2900, loss 1.4742028713226318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [35:27<00:00, 2127.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--training finished.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer = MyTrainer(model=model, trainset_size=len(edu_lst), config=CFG)\n",
    "best_res, best_state_dict = trainer.train(model=model, train_iter=train_iter, val_iter=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cf4ac776",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_results(save_file='../results/res.txt', save_title='', results=best_res)\n",
    "torch.save(best_state_dict, f\"../results/prompt_model_diag.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20682eb3-c57f-4d4c-90d1-7985a2dda89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.system('shutdown')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "f5970cc3159b15dedb1f3a2bddfb758e67a1721376b7397fa1f3df8941c2f173"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
